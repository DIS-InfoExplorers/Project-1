{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb1a79-2f45-4ed5-b3ab-48b5619234f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from numba import jit\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from annoy import AnnoyIndex\n",
    "import random\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from annoy import AnnoyIndex\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2ca11-c66e-4658-a85c-ea13dec7836c",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a395828-6cbf-4516-9106-fb9458d220d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_json('corpus.jsonl', lines=True).sort_values(by=[\"_id\"]).rename(columns={\"_id\": \"corpus-id\"})\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ddcf3e-aca1-4602-9c72-48d26c2ec00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = pd.read_json(path_or_buf='queries.jsonl', lines=True).sort_values(by=[\"_id\"])\n",
    "queries['text'] = queries['text'].str.strip()#.apply(tokenize)\n",
    "queries = queries.drop(columns=[\"metadata\"]).rename(columns={\"_id\": \"query-id\"})\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5ef2c-02ec-4d77-92b8-99bd8a06b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_corpus_train_map = pd.read_csv(\"task1_train.tsv\", sep=\"\\t\")\n",
    "query_corpus_train_map.sort_values(by=\"query-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f943ee-8ae8-4aff-b8eb-15a0717e8e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train = pd.merge(queries, query_corpus_train_map, left_on='query-id', right_on='query-id', how='inner').drop(columns=[ \"score\",\"corpus-id\"])\n",
    "queries_train_subset = queries_train.iloc[:7437, :]\n",
    "queries_train_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc517f81-bcb9-4023-8aec-2ff9bc0883ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"task1_test.tsv\", sep=\"\\t\")\n",
    "queries_test = pd.merge(queries, df_test, left_on='query-id', right_on='query-id', how='inner').drop(columns=[\"id\"])\n",
    "queries_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8505d-d729-4bb1-ba94-def1ccab8631",
   "metadata": {},
   "source": [
    "## Tools preparation & usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b3694-31d3-4d61-a635-15e21eed69b2",
   "metadata": {},
   "source": [
    "### TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65fc72f-a4f1-49b2-98d6-51743ee426ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def cosine_distance(u:np.ndarray, v:np.ndarray):\n",
    "    dot_products = v.dot(u.T)\n",
    "\n",
    "    # Compute norms\n",
    "    query_norm = np.linalg.norm(u)\n",
    "    corpus_norms = np.linalg.norm(v.toarray(), axis=1)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = dot_products.flatten() / (query_norm * corpus_norms + 1e-10)  # small value to avoid division by zero\n",
    "\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9626011-7c49-4591-9122-7888d7eb154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "vectorizer.fit(corpus[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634190e-30ba-4c3e-8941-70c412b1c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100  # maximum size of each chunk\n",
    "\n",
    "sub_corpus_list = [df for _, df in corpus.groupby(np.arange(len(corpus)) // n)]\n",
    "sub_corpus_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec07d6c2-93a7-4c38-963e-ae28c65d4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_matches(query):\n",
    "    query_vector = vectorizer.transform([query['text']])\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = np.array([1,2,3,4,5,6,7,8,9,10,11]) #linear_kernel(query_vector, sub_corpus_matrix).flatten()\n",
    "    # Get top k corpus indices from this sub-corpus\n",
    "    top_k_indices = cosine_similarities.argsort()[-10:][::-1]  # Here, k=10\n",
    "    \n",
    "    local_results = []\n",
    "    for index in top_k_indices:\n",
    "        local_results.append({\n",
    "            'query_id': query['query-id'],\n",
    "            'corpus_id': sub_corpus_df.iloc[index]['_id']\n",
    "        })\n",
    "    return local_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee33297-187b-4646-abe7-cf1830592e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for sub_corpus_df in sub_corpus_list:\n",
    "    print(\"Transforming\")\n",
    "    sub_corpus_matrix = vectorizer.fit_transform(sub_corpus_df[\"text\"])\n",
    "    print(\"Parallelizing\")\n",
    "    # Use ProcessPoolExecutor to parallelize the inner loop\n",
    "    count = 0\n",
    "    for _, query in queries_test.iterrows():\n",
    "        top_k_indices = find_top_k_matches(query)\n",
    "        results.extend(top_k_indices)\n",
    "        if count%100==0:\n",
    "            print(count)\n",
    "        count +=1\n",
    "        \n",
    "        \n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('query_corpus_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dc001-ddc1-4097-997f-b3766ff25229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of documents and queries\n",
    "documents = corpus\n",
    "qs = queries_test \n",
    "\n",
    "# 1. Vectorize the documents using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "doc_vectors = vectorizer.fit_transform(documents[\"text\"]).toarray()\n",
    "\n",
    "# 2. Build Annoy Index\n",
    "f = doc_vectors.shape[1]  # Number of dimensions of the vector\n",
    "t = AnnoyIndex(f, 'angular')  # Use 'angular' for cosine similarity\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    t.add_item(i, doc_vectors[i])\n",
    "\n",
    "t.build(50)  # 50 trees. Increase if needed.\n",
    "\n",
    "# 3. Query the index\n",
    "results = {}\n",
    "\n",
    "for query in qs:\n",
    "    query_vector = vectorizer.transform([query]).toarray()[0]\n",
    "    top10_indices = t.get_nns_by_vector(query_vector, 10)  # Find the top 10 document indices\n",
    "    results[query] = [documents[i] for i in top10_indices]\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f9224b-d53b-4c24-87cf-e284fb82f59b",
   "metadata": {},
   "source": [
    "### TEST 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405da4a-01c7-4538-86da-186ca9d093a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_qrs_to_docs(qrs, dcs):\n",
    "    \n",
    "    # 1. Hashing: Convert documents to hashed vectors\n",
    "    start = time.time()\n",
    "    hash_vectorizer = HashingVectorizer(n_features=2**20, stop_words='english', norm=None)\n",
    "    hashed_docs = hash_vectorizer.transform(dcs[\"text\"])\n",
    "    end = time.time()\n",
    "    print(f\"Hashing documents took {end - start} seconds.\")\n",
    "    \n",
    "    # 2. Dimensionality Reduction\n",
    "    start = time.time()\n",
    "    transformer = SparseRandomProjection(n_components=100)  # Reducing to 100 dimensions\n",
    "    reduced_docs = transformer.fit_transform(hashed_docs).toarray()\n",
    "    end = time.time()\n",
    "    print(f\"Dimensionality reduction took {end - start} seconds.\")\n",
    "    \n",
    "    # Build Annoy Index\n",
    "    start = time.time()\n",
    "    f = reduced_docs.shape[1]\n",
    "    t = AnnoyIndex(f, 'angular')\n",
    "    for i, vector in enumerate(reduced_docs):\n",
    "        t.add_item(i, vector)\n",
    "    t.build(50)\n",
    "    end = time.time()\n",
    "    print(f\"Building Annoy index took {end - start} seconds.\")\n",
    "    \n",
    "    # Query the index\n",
    "    results = {}\n",
    "    start = time.time()\n",
    "    for index, row in qrs.iterrows():\n",
    "        query_text = row[\"text\"]\n",
    "        hashed_query = hash_vectorizer.transform([query_text])\n",
    "        reduced_query = transformer.transform(hashed_query).toarray()[0]\n",
    "        top10_indices = t.get_nns_by_vector(reduced_query, 10)\n",
    "        \n",
    "        # Storing the _id from documents\n",
    "        results[row[\"query-id\"]] = [dcs.iloc[i][\"corpus-id\"] for i in top10_indices]\n",
    "    end = time.time()\n",
    "    print(f\"Querying the index took {end - start} seconds.\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5a333-6158-4213-bb0b-dce51cef9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = map_qrs_to_docs(queries_train_subset, corpus)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28d2fe-9195-4d84-b18f-07add621bea5",
   "metadata": {},
   "source": [
    "### TEST 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03e6d3-90e5-4bbe-b456-0489cb2d6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def build_inverted_index(docs):\n",
    "    index = defaultdict(set)\n",
    "    for doc_id, row in docs.iterrows():\n",
    "        # Remove punctuation and convert to lowercase\n",
    "        clean_text = remove_punctuation(row[\"text\"].lower())\n",
    "        for word in clean_text.split():\n",
    "            index[word].add(doc_id)\n",
    "    return index\n",
    "    \n",
    "def filter_docs(query, index):\n",
    "    relevant_doc_ids = set()\n",
    "    for word in query.split():\n",
    "        relevant_doc_ids.update(index.get(word, set()))\n",
    "    return relevant_doc_ids\n",
    "\n",
    "start = time.time()\n",
    "inverted_index = build_inverted_index(corpus)\n",
    "end = time.time()\n",
    "print(f\"Indexing took {end - start} seconds.\")\n",
    "inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98787daf-d411-44d0-8623-d3ac27fcc868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_from_index(inverted_index):\n",
    "    # Use a predefined list of stopwords (ENGLISH_STOP_WORDS from sklearn here)\n",
    "    for stopword in ENGLISH_STOP_WORDS:\n",
    "        if stopword in inverted_index:\n",
    "            del inverted_index[stopword]\n",
    "    return inverted_index\n",
    "inverted_index = remove_stopwords_from_index(inverted_index)\n",
    "inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a65b4-4c5d-4dcc-bef4-914de3810f48",
   "metadata": {},
   "source": [
    "## JUNK & OTHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ebce8-130c-4fd6-b711-01ebc8b70b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for idx, df in enumerate(sub_corpus_list):\n",
    "    # Transform the text in the dataframe using the vectorizer\n",
    "    tfidf_matrix = vectorizer.transform(df[\"text\"])\n",
    "    \n",
    "    # Convert the sparse matrix to a dense DataFrame\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    tfidf_df.to_csv(f'tfidf_matrix_{idx}.csv', index=False)\"\"\"\n",
    "\"\"\"\n",
    "for idx, df in enumerate(sub_corpus_list):\n",
    "    print(\"Transforming\")\n",
    "    tfidf_matrix = vectorizer.transform(df[\"text\"])\n",
    "    #print(\"framing\")\n",
    "    #tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    #print(\"storing\")\n",
    "    #tfidf_df.to_parquet(f'tfidf_matrix_{idx}.parquet')\n",
    "    for idx, q_text  in queries:\n",
    "        query_feature = tf.transform([query])\n",
    "        cosine_similarities = linear_kernel(query_feature,corpus_feature).flatten()\n",
    "        top_10 = np.argpartition(cosine_similarities,-5)[-5:]\"\"\"  \n",
    "# Placeholder for the results\n",
    "\n",
    "\"\"\"\n",
    "results = []\n",
    "# Assuming list_of_dfs is the list of your sub-corpuses created earlier\n",
    "for sub_corpus_df in sub_corpus_list:\n",
    "    print(\"Transform sub\")\n",
    "    sub_corpus_matrix = vectorizer.transform(sub_corpus_df[\"text\"])\n",
    "    \n",
    "    for _, query in queries.iterrows():\n",
    "        print(\"Treat query\")\n",
    "        query_vector = vectorizer.transform([query['text']])\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        cosine_similarities = linear_kernel(query_vector, sub_corpus_matrix).flatten()\n",
    "        \n",
    "        # Get top k corpus indices from this sub-corpus\n",
    "        top_k_indices = cosine_similarities.argsort()[-10:][::-1]  # Here, k=10\n",
    "        \n",
    "        for index in top_k_indices:\n",
    "            results.append({\n",
    "                'query_id': query['_id'],\n",
    "                'corpus_id': sub_corpus_df.iloc[index]['_id']\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('query_corpus_mapping.csv', index=False)\n",
    "\"\"\"\n",
    "def find_top_k_matches(query):\n",
    "    print(\"Transform query\")\n",
    "    query_vector = vectorizer.transform([query['text']])\n",
    "\n",
    "    print(\"Cosine Sim\")\n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = linear_kernel(query_vector, sub_corpus_matrix).flatten()\n",
    "\n",
    "    print(\"K vals\")\n",
    "    # Get top k corpus indices from this sub-corpus\n",
    "    top_k_indices = cosine_similarities.argsort()[-10:][::-1]  # Here, k=10\n",
    "    \n",
    "    local_results = []\n",
    "    for index in top_k_indices:\n",
    "        local_results.append({\n",
    "            'query_id': query['query_id'],\n",
    "            'corpus_id': sub_corpus_df.iloc[index]['corpus_id']\n",
    "        })\n",
    "    print(\"Finished query !\")\n",
    "    return local_results\n",
    "\"\"\"   \n",
    "def find_top_k_matches(query):\n",
    "    query_vector = vectorizer.transform([query['text']])\n",
    "    cosine_similarities = []\n",
    "\n",
    "    for idx in range(sub_corpus_matrix.shape[0]):\n",
    "        corpus_vector = sub_corpus_matrix[idx].toarray().flatten()\n",
    "        similarity = cosine_distance(query_vector.toarray().flatten(), corpus_vector)\n",
    "        cosine_similarities.append(similarity)\n",
    "\n",
    "    # Get top k corpus indices from this sub-corpus\n",
    "    top_k_indices = np.argsort(cosine_similarities)[-10:][::-1]  # Here, k=10\n",
    "    \n",
    "    local_results = []\n",
    "    for index in top_k_indices:\n",
    "        local_results.append({\n",
    "            'query_id': query['query_id'],\n",
    "            'corpus_id': sub_corpus_df.iloc[index]['corpus_id']\n",
    "        })\n",
    "    \n",
    "    return local_results\n",
    "\"\"\"\n",
    "results = []\n",
    "\n",
    "# Assuming list_of_dfs is the list of your sub-corpuses created earlier\n",
    "for sub_corpus_df in sub_corpus_list:\n",
    "    print(\"Transforming\")\n",
    "    sub_corpus_matrix = vectorizer.transform(sub_corpus_df[\"text\"])\n",
    "    print(\"Parallelizing\")\n",
    "    # Use ProcessPoolExecutor to parallelize the inner loop\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results.extend(executor.map(find_top_k_matches, queries_test.iterrows()))\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('query_corpus_mapping.csv', index=False)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b17c5-58fe-49b1-9d8e-e2e1e38fde67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75435971-6d5e-48e5-8660-f05b51052077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
