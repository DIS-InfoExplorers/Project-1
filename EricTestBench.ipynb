{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "31eb1a79-2f45-4ed5-b3ab-48b5619234f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from numba import jit\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from annoy import AnnoyIndex\n",
    "import random\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from annoy import AnnoyIndex\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import string\n",
    "from tfidf import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2ca11-c66e-4658-a85c-ea13dec7836c",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1a395828-6cbf-4516-9106-fb9458d220d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000000</th>\n",
       "      <td>0</td>\n",
       "      <td>The presence of communication amid scientific ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966376</th>\n",
       "      <td>8</td>\n",
       "      <td>In June 1942, the United States Army Corps of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468831</th>\n",
       "      <td>12</td>\n",
       "      <td>Tutorial: Introduction to Restorative Justice....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000001</th>\n",
       "      <td>16</td>\n",
       "      <td>The approach is based on a theory of justice t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306952</th>\n",
       "      <td>23</td>\n",
       "      <td>Phloem is a conductive (or vascular) tissue fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950989</th>\n",
       "      <td>8841780</td>\n",
       "      <td>Wolves don't hide. They don't even live in cav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395590</th>\n",
       "      <td>8841787</td>\n",
       "      <td>The UNHCR Country Representative in Kenya. Str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93101</th>\n",
       "      <td>8841790</td>\n",
       "      <td>2. Describe the misery at Kakuma. 3. Compariso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669122</th>\n",
       "      <td>8841800</td>\n",
       "      <td>Following the death of his employer and mentor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593471</th>\n",
       "      <td>8841801</td>\n",
       "      <td>Presently, Puerto Rico holds the most titles f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1471406 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         corpus-id                                               text\n",
       "1000000          0  The presence of communication amid scientific ...\n",
       "966376           8  In June 1942, the United States Army Corps of ...\n",
       "468831          12  Tutorial: Introduction to Restorative Justice....\n",
       "1000001         16  The approach is based on a theory of justice t...\n",
       "306952          23  Phloem is a conductive (or vascular) tissue fo...\n",
       "...            ...                                                ...\n",
       "950989     8841780  Wolves don't hide. They don't even live in cav...\n",
       "395590     8841787  The UNHCR Country Representative in Kenya. Str...\n",
       "93101      8841790  2. Describe the misery at Kakuma. 3. Compariso...\n",
       "669122     8841800  Following the death of his employer and mentor...\n",
       "593471     8841801  Presently, Puerto Rico holds the most titles f...\n",
       "\n",
       "[1471406 rows x 2 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_json('data/corpus.jsonl', lines=True).sort_values(by=[\"_id\"]).rename(columns={\"_id\": \"corpus-id\"})\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fbc3b-eb91-42df-af48-21ae693ff074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a1ddcf3e-aca1-4602-9c72-48d26c2ec00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>506217</th>\n",
       "      <td>2</td>\n",
       "      <td>Androgen receptor define</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65864</th>\n",
       "      <td>3</td>\n",
       "      <td>Another name for the primary visual cortex is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372466</th>\n",
       "      <td>4</td>\n",
       "      <td>Defining alcoholism as a disease is associated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326447</th>\n",
       "      <td>5</td>\n",
       "      <td>ECT is a treatment that is used for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117580</th>\n",
       "      <td>6</td>\n",
       "      <td>Ebolavirus is an enveloped virus, which means</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158901</th>\n",
       "      <td>1185863</td>\n",
       "      <td>why did rachel carson die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83120</th>\n",
       "      <td>1185864</td>\n",
       "      <td>definition of ramen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7634</th>\n",
       "      <td>1185865</td>\n",
       "      <td>amex india customer care number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1185868</td>\n",
       "      <td>_________ justice is designed to repair the ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1185869</td>\n",
       "      <td>)what was the immediate impact of the success ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509962 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        query-id                                               text\n",
       "506217         2                           Androgen receptor define\n",
       "65864          3      Another name for the primary visual cortex is\n",
       "372466         4  Defining alcoholism as a disease is associated...\n",
       "326447         5                ECT is a treatment that is used for\n",
       "117580         6      Ebolavirus is an enveloped virus, which means\n",
       "...          ...                                                ...\n",
       "158901   1185863                          why did rachel carson die\n",
       "83120    1185864                                definition of ramen\n",
       "7634     1185865                    amex india customer care number\n",
       "1        1185868  _________ justice is designed to repair the ha...\n",
       "0        1185869  )what was the immediate impact of the success ...\n",
       "\n",
       "[509962 rows x 2 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = pd.read_json(path_or_buf='data/queries.jsonl', lines=True).sort_values(by=[\"_id\"])\n",
    "queries['text'] = queries['text'].str.strip()#.apply(tokenize)\n",
    "queries = queries.drop(columns=[\"metadata\"]).rename(columns={\"_id\": \"query-id\"})\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "efd5ef2c-02ec-4d77-92b8-99bd8a06b9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70257</th>\n",
       "      <td>3</td>\n",
       "      <td>1142680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395137</th>\n",
       "      <td>4</td>\n",
       "      <td>5613529</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346352</th>\n",
       "      <td>5</td>\n",
       "      <td>4956428</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125307</th>\n",
       "      <td>6</td>\n",
       "      <td>1931409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66896</th>\n",
       "      <td>8</td>\n",
       "      <td>1094214</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169115</th>\n",
       "      <td>1185863</td>\n",
       "      <td>2545716</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88577</th>\n",
       "      <td>1185864</td>\n",
       "      <td>1408016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8141</th>\n",
       "      <td>1185865</td>\n",
       "      <td>229186</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1185868</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1185869</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>532751 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        query-id  corpus-id  score\n",
       "70257          3    1142680      1\n",
       "395137         4    5613529      1\n",
       "346352         5    4956428      1\n",
       "125307         6    1931409      1\n",
       "66896          8    1094214      1\n",
       "...          ...        ...    ...\n",
       "169115   1185863    2545716      1\n",
       "88577    1185864    1408016      1\n",
       "8141     1185865     229186      1\n",
       "1        1185868         16      1\n",
       "0        1185869          0      1\n",
       "\n",
       "[532751 rows x 3 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_corpus_train_map = pd.read_csv(\"data/task1_train.tsv\", sep=\"\\t\")\n",
    "query_corpus_train_map.sort_values(by=\"query-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "13f943ee-8ae8-4aff-b8eb-15a0717e8e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Another name for the primary visual cortex is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Defining alcoholism as a disease is associated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>ECT is a treatment that is used for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Ebolavirus is an enveloped virus, which means</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>In humans, the normal set point for body tempe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7432</th>\n",
       "      <td>18204</td>\n",
       "      <td>anger is fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7433</th>\n",
       "      <td>18205</td>\n",
       "      <td>anger management definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7434</th>\n",
       "      <td>18208</td>\n",
       "      <td>angie baby meaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7435</th>\n",
       "      <td>18209</td>\n",
       "      <td>angie lindvall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7436</th>\n",
       "      <td>18211</td>\n",
       "      <td>angies list customer service phone number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7437 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query-id                                               text\n",
       "0            3      Another name for the primary visual cortex is\n",
       "1            4  Defining alcoholism as a disease is associated...\n",
       "2            5                ECT is a treatment that is used for\n",
       "3            6      Ebolavirus is an enveloped virus, which means\n",
       "4            8  In humans, the normal set point for body tempe...\n",
       "...        ...                                                ...\n",
       "7432     18204                                      anger is fear\n",
       "7433     18205                        anger management definition\n",
       "7434     18208                                 angie baby meaning\n",
       "7435     18209                                     angie lindvall\n",
       "7436     18211          angies list customer service phone number\n",
       "\n",
       "[7437 rows x 2 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_train = pd.merge(queries, query_corpus_train_map, left_on='query-id', right_on='query-id', how='inner').drop(columns=[ \"score\",\"corpus-id\"])\n",
    "queries_train_subset = queries_train.iloc[:7437, :]\n",
    "queries_train_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bc517f81-bcb9-4023-8aec-2ff9bc0883ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Androgen receptor define</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1215</td>\n",
       "      <td>3 levels of government in canada and their res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1288</td>\n",
       "      <td>3/5 of 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1576</td>\n",
       "      <td>60x40 slab cost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2235</td>\n",
       "      <td>Bethel University was founded in what year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7432</th>\n",
       "      <td>1102335</td>\n",
       "      <td>why do people buy cars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7433</th>\n",
       "      <td>1102351</td>\n",
       "      <td>why do jefferson and stanton include these sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7434</th>\n",
       "      <td>1102390</td>\n",
       "      <td>why do children get aggressive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7435</th>\n",
       "      <td>1102393</td>\n",
       "      <td>why do celebrate st patrick's day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7436</th>\n",
       "      <td>1102400</td>\n",
       "      <td>why do bears hibernate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7437 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query-id                                               text\n",
       "0            2                           Androgen receptor define\n",
       "1         1215  3 levels of government in canada and their res...\n",
       "2         1288                                          3/5 of 60\n",
       "3         1576                                    60x40 slab cost\n",
       "4         2235         Bethel University was founded in what year\n",
       "...        ...                                                ...\n",
       "7432   1102335                             why do people buy cars\n",
       "7433   1102351  why do jefferson and stanton include these sim...\n",
       "7434   1102390                     why do children get aggressive\n",
       "7435   1102393                  why do celebrate st patrick's day\n",
       "7436   1102400                             why do bears hibernate\n",
       "\n",
       "[7437 rows x 2 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"data/task1_test.tsv\", sep=\"\\t\")\n",
    "queries_test = pd.merge(queries, df_test, left_on='query-id', right_on='query-id', how='inner').drop(columns=[\"id\"])\n",
    "queries_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8505d-d729-4bb1-ba94-def1ccab8631",
   "metadata": {},
   "source": [
    "## Tools preparation & usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768acd7-423c-4d03-9556-b1d84b4431ee",
   "metadata": {},
   "source": [
    "### New Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7ca40aaa-daeb-4054-aa51-b0b5438d0d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         corpus-id                                               text\n",
      "1061928    1142680  The primary (parts of the cortex that receive ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Another name for the primary visual cortex is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query-id                                           text\n",
       "0         3  Another name for the primary visual cortex is"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_corpus = corpus[corpus[\"corpus-id\"]== 1142680]\n",
    "correct_query = queries_train_subset[queries_train_subset[\"query-id\"] == 3]\n",
    "print(correct_corpus)\n",
    "correct_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "f13e3400-31f6-4dae-9934-86317387f674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['primari',\n",
       " 'part',\n",
       " 'cortex',\n",
       " 'receiv',\n",
       " 'sensori',\n",
       " 'input',\n",
       " 'thalamu',\n",
       " 'visual',\n",
       " 'cortex',\n",
       " 'also',\n",
       " 'known',\n",
       " 'v',\n",
       " 'v',\n",
       " 'isual',\n",
       " 'area',\n",
       " 'one',\n",
       " 'striat',\n",
       " 'cortex',\n",
       " 'extrastri',\n",
       " 'area',\n",
       " 'consist',\n",
       " 'visual',\n",
       " 'area',\n",
       " 'two',\n",
       " 'v',\n",
       " 'three',\n",
       " 'v',\n",
       " 'four',\n",
       " 'v',\n",
       " 'five',\n",
       " 'v',\n",
       " 'primari',\n",
       " 'visual',\n",
       " 'cortex',\n",
       " 'best',\n",
       " 'studi',\n",
       " 'visual',\n",
       " 'area',\n",
       " 'brain',\n",
       " 'mammal',\n",
       " 'studi',\n",
       " 'locat',\n",
       " 'posterior',\n",
       " 'pole',\n",
       " 'occipit',\n",
       " 'cortex',\n",
       " 'occipit',\n",
       " 'cortex',\n",
       " 'respons',\n",
       " 'process',\n",
       " 'visual',\n",
       " 'stimulu']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Optimized text preprocessing function.\"\"\"\n",
    "    \n",
    "    # Cleaning\n",
    "    text = HTML_PATTERN.sub(\"\", text)\n",
    "    text = NON_ASCII_DIGITS_PATTERN.sub(\" \", text)\n",
    "    text = NON_ASCII_CHARS_PATTERN.sub('', text)\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords, and then perform Stemming and Lemmatization\n",
    "    preprocessed_tokens = [\n",
    "        STEMMER.stem(LEMMATIZER.lemmatize(word))\n",
    "        for word in tokens\n",
    "        if word.lower() not in STOPWORDS_SET\n",
    "    ]\n",
    "    \n",
    "    return preprocessed_tokens\n",
    "preprocess_text(correct_corpus.iloc[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "c99e0ab7-dd19-4dd9-aa9b-1eb69ac046cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The presence of communication amid scientific ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>In June 1942, the United States Army Corps of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>Tutorial: Introduction to Restorative Justice....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>The approach is based on a theory of justice t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>Phloem is a conductive (or vascular) tissue fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1142680</td>\n",
       "      <td>The primary (parts of the cortex that receive ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   corpus-id                                               text\n",
       "0          0  The presence of communication amid scientific ...\n",
       "1          8  In June 1942, the United States Army Corps of ...\n",
       "2         12  Tutorial: Introduction to Restorative Justice....\n",
       "3         16  The approach is based on a theory of justice t...\n",
       "4         23  Phloem is a conductive (or vascular) tissue fo...\n",
       "5    1142680  The primary (parts of the cortex that receive ..."
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_docs = corpus.iloc[:5]\n",
    "subset_docs = pd.concat([subset_docs, correct_corpus], ignore_index=True)\n",
    "subset_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f09f36cd-49ad-4b3d-a83a-d8cfaf64cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_tfidf_dataframe(documents, vocabulary):\n",
    "    # Create a list of dictionaries with term frequencies\n",
    "    list_of_dicts = [Counter(doc) for doc in documents]\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(list_of_dicts).fillna(0)\n",
    "    \n",
    "    # Reorder columns according to the vocabulary and fill missing columns with 0\n",
    "    df = df.reindex(columns=vocabulary, fill_value=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "dadcedc9-15aa-4179-a36a-541788894d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process docs ...\n",
      "Create vocab ...\n",
      "Compute tf ...\n",
      "Compute idf ...\n",
      "Compute tf-idf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def tfidf_with_pandas(corpus):\n",
    "    # Parallel tokenization and preprocessing\n",
    "    print(\"Process docs ...\")\n",
    "    documents = parallel_preprocess_texts(corpus)\n",
    "\n",
    "    print(\"Create vocab ...\")\n",
    "    # Create the vocabulary\n",
    "    vocabulary = list(set(word for doc in documents for word in doc))\n",
    "    vocabulary.sort()\n",
    "    \n",
    "    # Use the helper function to create and populate the DataFrame for term frequencies\n",
    "    print(\"Compute tf ...\")\n",
    "    df = populate_tfidf_dataframe(documents, vocabulary)\n",
    "\n",
    "    # Compute IDF values\n",
    "    print(\"Compute idf ...\")\n",
    "    doc_count = len(documents)\n",
    "    idf = df[df > 0].count().apply(lambda x: log(doc_count / x))\n",
    "\n",
    "        \n",
    "    # Compute TF-IDF values\n",
    "    print(\"Compute tf-idf ...\")\n",
    "    tfidf_df = df.apply(lambda x: x / x.sum(), axis=1).multiply(idf)\n",
    "    \n",
    "    return original_documents, documents, tfidf_df, vocabulary, idf\n",
    "original_documents, documents, tfidf_df, vocabulary, idf = tfidf_with_pandas(subset_docs[\"text\"])\n",
    "#tfidf_with_pandas(subset_docs[\"text\"])[\"cortex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a801b752-646d-430d-bfa8-1a2f6bd7082a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000000\n",
       "1    0.000000\n",
       "2    0.000000\n",
       "3    0.000000\n",
       "4    0.000000\n",
       "5    0.206741\n",
       "Name: cortex, dtype: float64"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df[\"cortex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "a4de42a5-a85b-40d8-8e3c-d1a649f8b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'anoth': 1, 'name': 1, 'primari': 1, 'visual': 1, 'cortex': 1})\n",
      "['accomplish', 'account', 'achiev', 'also', 'amid', 'approach', 'area', 'armi', 'atom', 'base', 'behaviour', 'best', 'bomb', 'brain', 'carri', 'caus', 'cloud', 'commun', 'conduct', 'consid', 'consist', 'cooper', 'corp', 'cortex', 'crime', 'crimin', 'dialogu', 'emphas', 'engin', 'engineersbegan', 'equal', 'extrastri', 'five', 'foster', 'found', 'four', 'glucos', 'govern', 'hang', 'harm', 'highest', 'hundr', 'identifi', 'import', 'impress', 'includ', 'individu', 'innoc', 'input', 'intellect', 'introduct', 'involv', 'isual', 'june', 'justic', 'known', 'lead', 'leaf', 'life', 'locat', 'mammal', 'manhattan', 'meant', 'mind', 'name', 'obliter', 'occipit', 'offend', 'offens', 'one', 'part', 'peopl', 'phloem', 'photosynthesi', 'plant', 'pole', 'posterior', 'practic', 'presenc', 'primari', 'process', 'product', 'program', 'project', 'purpos', 'rate', 'rather', 'receiv', 'reflect', 'relationship', 'repair', 'research', 'respond', 'respons', 'restor', 'satisfact', 'scientif', 'secret', 'sensori', 'shown', 'stakehold', 'state', 'step', 'stimulu', 'striat', 'studi', 'success', 'sucros', 'take', 'thalamu', 'theori', 'thousand', 'three', 'tissu', 'tradit', 'transform', 'truli', 'tutori', 'two', 'unit', 'v', 'vascular', 'victim', 'visual', 'wrongdo']\n"
     ]
    }
   ],
   "source": [
    "def vectorize_query(query, vocabulary, idf):\n",
    "    \"\"\"Convert the query into its TF-IDF vector.\"\"\"\n",
    "    query_tf = Counter(preprocess_text(query))\n",
    "    print(query_tf)\n",
    "    query_vector = [query_tf.get(term, 0) * idf[term] for term in vocabulary]\n",
    "    return query_vector\n",
    "\n",
    "vector_query = vectorize_query(correct_query[\"text\"].iloc[0], vocabulary, idf)\n",
    "#query_vector = {k: v for k, v in vector_query.items() if v != 0}\n",
    "\n",
    "#rint(query_vector)\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "2b042041-c32f-4d43-ab0e-37a03f675c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.06636146 0.         0.06636146 0.\n",
      "  0.         0.         0.04068934 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.06636146 0.02567212\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.06636146 0.\n",
      "  0.06636146 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.06636146 0.         0.         0.06636146\n",
      "  0.         0.06636146 0.06636146 0.         0.         0.06636146\n",
      "  0.         0.06636146 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.06636146 0.\n",
      "  0.         0.04068934 0.06636146 0.06636146 0.         0.06636146\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06636146 0.         0.         0.         0.         0.04068934\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.06636146 0.         0.         0.         0.\n",
      "  0.13272292 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.13272292 0.\n",
      "  0.         0.         0.         0.06636146 0.         0.\n",
      "  0.         0.         0.06636146 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14931329 0.09155102 0.         0.         0.\n",
      "  0.14931329 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.14931329 0.\n",
      "  0.         0.         0.         0.         0.         0.14931329\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.14931329\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.09155102 0.         0.         0.14931329 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.09155102\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14931329 0.         0.         0.         0.09155102\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.14931329\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.03895129 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.03895129 0.02388288\n",
      "  0.         0.         0.         0.03895129 0.         0.03013683\n",
      "  0.         0.         0.         0.03895129 0.         0.\n",
      "  0.04776575 0.03895129 0.         0.03895129 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.03895129 0.         0.07790259 0.         0.\n",
      "  0.03895129 0.         0.         0.03895129 0.         0.\n",
      "  0.         0.         0.03895129 0.03895129 0.         0.\n",
      "  0.07164863 0.         0.03895129 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.03895129\n",
      "  0.         0.         0.         0.         0.         0.03895129\n",
      "  0.         0.         0.02388288 0.         0.03895129 0.\n",
      "  0.03895129 0.         0.         0.         0.03895129 0.07790259\n",
      "  0.07790259 0.         0.07790259 0.         0.07164863 0.\n",
      "  0.         0.         0.         0.         0.07790259 0.\n",
      "  0.03895129 0.         0.         0.         0.         0.\n",
      "  0.03895129 0.         0.02388288 0.         0.         0.\n",
      "  0.03895129 0.07790259 0.         0.03895129 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.07167038 0.         0.         0.         0.07167038\n",
      "  0.         0.         0.         0.07167038 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.02772589\n",
      "  0.         0.07167038 0.         0.         0.         0.\n",
      "  0.04394449 0.         0.07167038 0.         0.         0.\n",
      "  0.         0.         0.         0.07167038 0.         0.\n",
      "  0.         0.         0.         0.         0.07167038 0.\n",
      "  0.         0.         0.         0.         0.07167038 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.08788898 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.14334076 0.07167038 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.07167038 0.07167038 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.04394449 0.07167038\n",
      "  0.         0.         0.         0.07167038 0.         0.04394449\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.04394449 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.14334076 0.         0.07167038]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11945063 0.         0.         0.\n",
      "  0.11945063 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11945063 0.\n",
      "  0.11945063 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.11945063 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.07324082 0.\n",
      "  0.23890126 0.11945063 0.23890126 0.         0.         0.\n",
      "  0.         0.         0.         0.11945063 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.11945063\n",
      "  0.         0.         0.         0.         0.         0.11945063\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.11945063 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.03445691 0.         0.\n",
      "  0.13782765 0.         0.         0.         0.         0.02112716\n",
      "  0.         0.03445691 0.         0.         0.         0.\n",
      "  0.         0.         0.03445691 0.         0.         0.20674148\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.03445691 0.03445691 0.         0.         0.03445691\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.03445691 0.         0.         0.         0.03445691 0.\n",
      "  0.         0.03445691 0.         0.         0.         0.03445691\n",
      "  0.03445691 0.         0.         0.         0.         0.\n",
      "  0.06891383 0.         0.         0.03445691 0.02112716 0.\n",
      "  0.         0.         0.         0.03445691 0.03445691 0.\n",
      "  0.         0.06891383 0.02112716 0.         0.         0.\n",
      "  0.         0.         0.         0.03445691 0.         0.\n",
      "  0.         0.         0.         0.03445691 0.         0.\n",
      "  0.         0.         0.03445691 0.         0.         0.\n",
      "  0.         0.03445691 0.03445691 0.06891383 0.         0.\n",
      "  0.         0.03445691 0.         0.         0.03445691 0.\n",
      "  0.         0.         0.         0.         0.03445691 0.\n",
      "  0.20674148 0.         0.         0.17228456 0.        ]]\n",
      "[0.        0.2675335 0.        0.        0.        0.8026005]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 1, 0, 2, 3, 4])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch_query(tfidf_matrix_normalized, query_vectors):\n",
    "    \"\"\"Process multiple queries and return ranked document indices for each query.\"\"\"\n",
    "    # Compute cosine similarities using matrix operations\n",
    "    similarity_matrix = np.dot(query_vectors, tfidf_matrix_normalized.T)\n",
    "    print(similarity_matrix)\n",
    "    # Get document indices ranked by relevance for each query\n",
    "    ranked_doc_indices = np.argsort(-similarity_matrix)\n",
    "    return ranked_doc_indices\n",
    "print(tfidf_df.values)\n",
    "batch_query(np.array(tfidf_df.values), vector_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0f125-121a-487f-a57b-0e85b1e977dc",
   "metadata": {},
   "source": [
    "### New Test 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61039d72-89a7-4445-a35d-c2730f176a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/var/folders/ym/9zzn1f9j7c7dnh381g3rrm_m0000gn/T/ipykernel_17432/3251252348.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_docs[\"list\"] = parallel_preprocess_texts(subset_docs['text'])\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/var/folders/ym/9zzn1f9j7c7dnh381g3rrm_m0000gn/T/ipykernel_17432/3251252348.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mini_sub[\"list\"] = parallel_preprocess_texts(mini_sub['text'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>text</th>\n",
       "      <th>list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000000</th>\n",
       "      <td>0</td>\n",
       "      <td>The presence of communication amid scientific ...</td>\n",
       "      <td>[presenc, commun, amid, scientif, mind, equal,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966376</th>\n",
       "      <td>8</td>\n",
       "      <td>In June 1942, the United States Army Corps of ...</td>\n",
       "      <td>[june, unit, state, armi, corp, engineersbegan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468831</th>\n",
       "      <td>12</td>\n",
       "      <td>Tutorial: Introduction to Restorative Justice....</td>\n",
       "      <td>[tutori, introduct, restor, justic, restor, ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000001</th>\n",
       "      <td>16</td>\n",
       "      <td>The approach is based on a theory of justice t...</td>\n",
       "      <td>[approach, base, theori, justic, consid, crime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306952</th>\n",
       "      <td>23</td>\n",
       "      <td>Phloem is a conductive (or vascular) tissue fo...</td>\n",
       "      <td>[phloem, conduct, vascular, tissu, found, plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605287</th>\n",
       "      <td>51256</td>\n",
       "      <td>Common mergansers in the Palearctic region typ...</td>\n",
       "      <td>[common, mergans, palearct, region, typic, bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206126</th>\n",
       "      <td>51261</td>\n",
       "      <td>How long do they live? The record for the olde...</td>\n",
       "      <td>[long, live, record, oldest, common, mergans, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263763</th>\n",
       "      <td>51266</td>\n",
       "      <td>If you are using desktop you can easily find t...</td>\n",
       "      <td>[use, desktop, easili, find, serial, number, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205026</th>\n",
       "      <td>51272</td>\n",
       "      <td>Use the following steps find your model number...</td>\n",
       "      <td>[use, follow, step, find, model, number, use, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756500</th>\n",
       "      <td>51281</td>\n",
       "      <td>DSC is an automated calling system that is par...</td>\n",
       "      <td>[dsc, autom, call, system, part, global, marit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         corpus-id                                               text  \\\n",
       "1000000          0  The presence of communication amid scientific ...   \n",
       "966376           8  In June 1942, the United States Army Corps of ...   \n",
       "468831          12  Tutorial: Introduction to Restorative Justice....   \n",
       "1000001         16  The approach is based on a theory of justice t...   \n",
       "306952          23  Phloem is a conductive (or vascular) tissue fo...   \n",
       "...            ...                                                ...   \n",
       "605287       51256  Common mergansers in the Palearctic region typ...   \n",
       "206126       51261  How long do they live? The record for the olde...   \n",
       "263763       51266  If you are using desktop you can easily find t...   \n",
       "205026       51272  Use the following steps find your model number...   \n",
       "756500       51281  DSC is an automated calling system that is par...   \n",
       "\n",
       "                                                      list  \n",
       "1000000  [presenc, commun, amid, scientif, mind, equal,...  \n",
       "966376   [june, unit, state, armi, corp, engineersbegan...  \n",
       "468831   [tutori, introduct, restor, justic, restor, ju...  \n",
       "1000001  [approach, base, theori, justic, consid, crime...  \n",
       "306952   [phloem, conduct, vascular, tissu, found, plan...  \n",
       "...                                                    ...  \n",
       "605287   [common, mergans, palearct, region, typic, bre...  \n",
       "206126   [long, live, record, oldest, common, mergans, ...  \n",
       "263763   [use, desktop, easili, find, serial, number, b...  \n",
       "205026   [use, follow, step, find, model, number, use, ...  \n",
       "756500   [dsc, autom, call, system, part, global, marit...  \n",
       "\n",
       "[7000 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_docs = corpus.iloc[:7000]\n",
    "mini_sub = corpus.iloc[:5]\n",
    "#subset_docs[\"list\"] = subset_docs['text'].apply(lambda x: preprocess_text(x))\n",
    "#subset_docs[\"list\"]\n",
    "subset_docs[\"list\"] = parallel_preprocess_texts(subset_docs['text'])\n",
    "mini_sub[\"list\"] = parallel_preprocess_texts(mini_sub['text'])\n",
    "subset_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca1fb3f1-4dcc-4a8c-9ccd-e3c5404a51a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>text</th>\n",
       "      <th>list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000000</th>\n",
       "      <td>0</td>\n",
       "      <td>The presence of communication amid scientific ...</td>\n",
       "      <td>[presenc, commun, amid, scientif, mind, equal,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966376</th>\n",
       "      <td>8</td>\n",
       "      <td>In June 1942, the United States Army Corps of ...</td>\n",
       "      <td>[june, unit, state, armi, corp, engineersbegan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468831</th>\n",
       "      <td>12</td>\n",
       "      <td>Tutorial: Introduction to Restorative Justice....</td>\n",
       "      <td>[tutori, introduct, restor, justic, restor, ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000001</th>\n",
       "      <td>16</td>\n",
       "      <td>The approach is based on a theory of justice t...</td>\n",
       "      <td>[approach, base, theori, justic, consid, crime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306952</th>\n",
       "      <td>23</td>\n",
       "      <td>Phloem is a conductive (or vascular) tissue fo...</td>\n",
       "      <td>[phloem, conduct, vascular, tissu, found, plan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         corpus-id                                               text  \\\n",
       "1000000          0  The presence of communication amid scientific ...   \n",
       "966376           8  In June 1942, the United States Army Corps of ...   \n",
       "468831          12  Tutorial: Introduction to Restorative Justice....   \n",
       "1000001         16  The approach is based on a theory of justice t...   \n",
       "306952          23  Phloem is a conductive (or vascular) tissue fo...   \n",
       "\n",
       "                                                      list  \n",
       "1000000  [presenc, commun, amid, scientif, mind, equal,...  \n",
       "966376   [june, unit, state, armi, corp, engineersbegan...  \n",
       "468831   [tutori, introduct, restor, justic, restor, ju...  \n",
       "1000001  [approach, base, theori, justic, consid, crime...  \n",
       "306952   [phloem, conduct, vascular, tissu, found, plan...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8fda1c60-bc16-4a3a-9421-94666da49415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accomplish',\n",
       " 'account',\n",
       " 'achiev',\n",
       " 'amid',\n",
       " 'approach',\n",
       " 'armi',\n",
       " 'atom',\n",
       " 'base',\n",
       " 'behaviour',\n",
       " 'best',\n",
       " 'bomb',\n",
       " 'carri',\n",
       " 'caus',\n",
       " 'cloud',\n",
       " 'commun',\n",
       " 'conduct',\n",
       " 'consid',\n",
       " 'cooper',\n",
       " 'corp',\n",
       " 'crime',\n",
       " 'crimin',\n",
       " 'dialogu',\n",
       " 'emphas',\n",
       " 'engin',\n",
       " 'engineersbegan',\n",
       " 'equal',\n",
       " 'foster',\n",
       " 'found',\n",
       " 'glucos',\n",
       " 'govern',\n",
       " 'hang',\n",
       " 'harm',\n",
       " 'highest',\n",
       " 'hundr',\n",
       " 'identifi',\n",
       " 'import',\n",
       " 'impress',\n",
       " 'includ',\n",
       " 'individu',\n",
       " 'innoc',\n",
       " 'intellect',\n",
       " 'introduct',\n",
       " 'involv',\n",
       " 'june',\n",
       " 'justic',\n",
       " 'lead',\n",
       " 'leaf',\n",
       " 'life',\n",
       " 'manhattan',\n",
       " 'meant',\n",
       " 'mind',\n",
       " 'name',\n",
       " 'obliter',\n",
       " 'offend',\n",
       " 'offens',\n",
       " 'part',\n",
       " 'peopl',\n",
       " 'phloem',\n",
       " 'photosynthesi',\n",
       " 'plant',\n",
       " 'practic',\n",
       " 'presenc',\n",
       " 'process',\n",
       " 'product',\n",
       " 'program',\n",
       " 'project',\n",
       " 'purpos',\n",
       " 'rate',\n",
       " 'rather',\n",
       " 'reflect',\n",
       " 'relationship',\n",
       " 'repair',\n",
       " 'research',\n",
       " 'respond',\n",
       " 'restor',\n",
       " 'satisfact',\n",
       " 'scientif',\n",
       " 'secret',\n",
       " 'shown',\n",
       " 'stakehold',\n",
       " 'state',\n",
       " 'step',\n",
       " 'success',\n",
       " 'sucros',\n",
       " 'take',\n",
       " 'theori',\n",
       " 'thousand',\n",
       " 'tissu',\n",
       " 'tradit',\n",
       " 'transform',\n",
       " 'truli',\n",
       " 'tutori',\n",
       " 'unit',\n",
       " 'vascular',\n",
       " 'victim',\n",
       " 'wrongdo']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = list(set(word for doc in mini_sub[\"list\"] for word in doc))\n",
    "vocabulary.sort()\n",
    "vocabulary#[0]\n",
    "#print(subset_docs[subset_docs[\"list\"].apply(lambda x: 'aa' in x)].iloc[0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0bb82a53-8647-4794-ae83-6b84ef5623c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accomplish</th>\n",
       "      <th>account</th>\n",
       "      <th>achiev</th>\n",
       "      <th>amid</th>\n",
       "      <th>approach</th>\n",
       "      <th>armi</th>\n",
       "      <th>atom</th>\n",
       "      <th>base</th>\n",
       "      <th>behaviour</th>\n",
       "      <th>best</th>\n",
       "      <th>...</th>\n",
       "      <th>thousand</th>\n",
       "      <th>tissu</th>\n",
       "      <th>tradit</th>\n",
       "      <th>transform</th>\n",
       "      <th>truli</th>\n",
       "      <th>tutori</th>\n",
       "      <th>unit</th>\n",
       "      <th>vascular</th>\n",
       "      <th>victim</th>\n",
       "      <th>wrongdo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accomplish  account  achiev  amid  approach  armi  atom  base  behaviour  \\\n",
       "0         0.0      0.0     1.0   1.0       0.0   0.0   1.0   0.0        0.0   \n",
       "1         0.0      0.0     0.0   0.0       0.0   1.0   1.0   0.0        0.0   \n",
       "2         1.0      0.0     0.0   0.0       0.0   0.0   0.0   0.0        1.0   \n",
       "3         0.0      1.0     0.0   0.0       1.0   0.0   0.0   1.0        0.0   \n",
       "4         0.0      0.0     0.0   0.0       0.0   0.0   0.0   0.0        0.0   \n",
       "\n",
       "   best  ...  thousand  tissu  tradit  transform  truli  tutori  unit  \\\n",
       "0   0.0  ...       1.0    0.0     0.0        0.0    1.0     0.0   0.0   \n",
       "1   0.0  ...       0.0    0.0     0.0        0.0    0.0     0.0   1.0   \n",
       "2   1.0  ...       0.0    0.0     1.0        2.0    0.0     1.0   0.0   \n",
       "3   0.0  ...       0.0    0.0     0.0        0.0    0.0     0.0   0.0   \n",
       "4   0.0  ...       0.0    1.0     0.0        0.0    0.0     0.0   0.0   \n",
       "\n",
       "   vascular  victim  wrongdo  \n",
       "0       0.0     0.0      0.0  \n",
       "1       0.0     0.0      0.0  \n",
       "2       0.0     0.0      0.0  \n",
       "3       0.0     2.0      1.0  \n",
       "4       1.0     0.0      0.0  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pop_tfidf_dataframe(documents, vocabulary):\n",
    "    # Create a list of dictionaries with term frequencies\n",
    "    list_of_dicts = [Counter(doc) for doc in documents]\n",
    "    #print(list_of_dicts)\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(list_of_dicts).fillna(0)\n",
    "    \n",
    "    # Reorder columns according to the vocabulary and fill missing columns with 0\n",
    "    df = df.reindex(columns=vocabulary, fill_value=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "pop = pop_tfidf_dataframe(mini_sub['list'], vocabulary)\n",
    "pop.loc[:, (pop != 0).any(axis=0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "72f74a3b-3eab-4c7e-bd5a-003bb580f133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_count = len(mini_sub)\n",
    "doc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "26175dee-d32a-412d-920f-c47c85cb7381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    96.000000\n",
      "mean      1.429603\n",
      "std       0.365684\n",
      "min       0.000000\n",
      "25%       1.609438\n",
      "50%       1.609438\n",
      "75%       1.609438\n",
      "max       1.609438\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#gamma = 1/100.0 #PARAMETER TO MAKE THE idf more visible\n",
    "alpha = 0  # Smoothing parameter, you can experiment with this\n",
    "idf = pop.sum().apply(lambda x: log(doc_count / (x + alpha)))\n",
    "\n",
    "print(idf.describe())\n",
    "#non_one_columns = pop.columns[pop.sum() != 1]\n",
    "#print(pop.sum()[non_one_columns])\n",
    "#print(idf[non_one_columns])\n",
    "#log(5/pop[pop > 0].count()[\"theori\"])\n",
    "#log(5/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c07cf38b-84f6-4392-8af7-a63ef6ee9b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accomplish</th>\n",
       "      <th>account</th>\n",
       "      <th>achiev</th>\n",
       "      <th>amid</th>\n",
       "      <th>approach</th>\n",
       "      <th>armi</th>\n",
       "      <th>atom</th>\n",
       "      <th>base</th>\n",
       "      <th>behaviour</th>\n",
       "      <th>best</th>\n",
       "      <th>...</th>\n",
       "      <th>thousand</th>\n",
       "      <th>tissu</th>\n",
       "      <th>tradit</th>\n",
       "      <th>transform</th>\n",
       "      <th>truli</th>\n",
       "      <th>tutori</th>\n",
       "      <th>unit</th>\n",
       "      <th>vascular</th>\n",
       "      <th>victim</th>\n",
       "      <th>wrongdo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059609</td>\n",
       "      <td>0.059609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.033937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.13412</td>\n",
       "      <td>0.076358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.13412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034988</td>\n",
       "      <td>0.034988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034988</td>\n",
       "      <td>0.039839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034988</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064378</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073303</td>\n",
       "      <td>0.064378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.107296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accomplish   account    achiev      amid  approach     armi      atom  \\\n",
       "0    0.000000  0.000000  0.059609  0.059609  0.000000  0.00000  0.033937   \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.13412  0.076358   \n",
       "2    0.034988  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "3    0.000000  0.064378  0.000000  0.000000  0.064378  0.00000  0.000000   \n",
       "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "\n",
       "       base  behaviour      best  ...  thousand     tissu    tradit  \\\n",
       "0  0.000000   0.000000  0.000000  ...  0.059609  0.000000  0.000000   \n",
       "1  0.000000   0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2  0.000000   0.034988  0.034988  ...  0.000000  0.000000  0.034988   \n",
       "3  0.064378   0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4  0.000000   0.000000  0.000000  ...  0.000000  0.107296  0.000000   \n",
       "\n",
       "   transform     truli    tutori     unit  vascular    victim   wrongdo  \n",
       "0   0.000000  0.059609  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "1   0.000000  0.000000  0.000000  0.13412  0.000000  0.000000  0.000000  \n",
       "2   0.039839  0.000000  0.034988  0.00000  0.000000  0.000000  0.000000  \n",
       "3   0.000000  0.000000  0.000000  0.00000  0.000000  0.073303  0.064378  \n",
       "4   0.000000  0.000000  0.000000  0.00000  0.107296  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pop.apply(lambda x: x / x.sum(), axis=1).multiply(idf)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "20ce4dff-d169-4708-9266-4161bf5e4b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericsaikali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/var/folders/ym/9zzn1f9j7c7dnh381g3rrm_m0000gn/T/ipykernel_17432/3450912701.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  min_query['list'] = parallel_preprocess_texts(min_query['text'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>text</th>\n",
       "      <th>list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Another name for the primary visual cortex is</td>\n",
       "      <td>[anoth, name, primari, visual, cortex]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Defining alcoholism as a disease is associated...</td>\n",
       "      <td>[defin, alcohol, diseas, associ, jellinek]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>ECT is a treatment that is used for</td>\n",
       "      <td>[ect, treatment, use]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Ebolavirus is an enveloped virus, which means</td>\n",
       "      <td>[ebolaviru, envelop, viru, mean]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>In humans, the normal set point for body tempe...</td>\n",
       "      <td>[human, normal, set, point, bodi, temperatur]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query-id                                               text  \\\n",
       "0         3      Another name for the primary visual cortex is   \n",
       "1         4  Defining alcoholism as a disease is associated...   \n",
       "2         5                ECT is a treatment that is used for   \n",
       "3         6      Ebolavirus is an enveloped virus, which means   \n",
       "4         8  In humans, the normal set point for body tempe...   \n",
       "\n",
       "                                            list  \n",
       "0         [anoth, name, primari, visual, cortex]  \n",
       "1     [defin, alcohol, diseas, associ, jellinek]  \n",
       "2                          [ect, treatment, use]  \n",
       "3               [ebolaviru, envelop, viru, mean]  \n",
       "4  [human, normal, set, point, bodi, temperatur]  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_query = queries_train_subset.iloc[:5]\n",
    "min_query['list'] = parallel_preprocess_texts(min_query['text'])\n",
    "min_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b9166f46-0f44-4d2f-a1f5-bfd7a28efdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "Name: list, dtype: object"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize_query(query, vocabulary, idf):\n",
    "    \"\"\"Convert the query into its TF-IDF vector.\"\"\"\n",
    "    query_tf = Counter(query)\n",
    "    query_vector = [query_tf.get(term, 0) * idf[term] for term in vocabulary]\n",
    "    return np.array(query_vector)\n",
    "vects = min_query[\"list\"].apply(lambda x: vectorize_query(x, vocabulary, idf))\n",
    "vects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "823f251f-a5a2-42a3-bee6-9b11c6151dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 96)\n",
      "(5, 96)\n",
      "(5, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 2, 3, 4],\n",
       "       [0, 1, 2, 3, 4],\n",
       "       [0, 1, 2, 3, 4],\n",
       "       [0, 1, 2, 3, 4],\n",
       "       [0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def batch_query(tfidf_matrix_normalized, query_vectors):\n",
    "    \"\"\"Process multiple queries and return ranked document indices for each query.\"\"\"\n",
    "    # Compute cosine similarities using matrix operations\n",
    "    print(tfidf_matrix_normalized.shape)\n",
    "    print(query_vectors.shape)\n",
    "    similarity_matrix = np.dot(query_vectors, tfidf_matrix_normalized.T)\n",
    "    print(similarity_matrix.shape)\n",
    "    # Get document indices ranked by relevance for each query\n",
    "    ranked_doc_indices = np.argsort(-similarity_matrix)\n",
    "    \n",
    "    return ranked_doc_indices\n",
    "array_2d = np.vstack(vects)\n",
    "batch_query(np.array(tfidf_df), np.array(array_2d))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b3694-31d3-4d61-a635-15e21eed69b2",
   "metadata": {},
   "source": [
    "### TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65fc72f-a4f1-49b2-98d6-51743ee426ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def cosine_distance(u:np.ndarray, v:np.ndarray):\n",
    "    dot_products = v.dot(u.T)\n",
    "\n",
    "    # Compute norms\n",
    "    query_norm = np.linalg.norm(u)\n",
    "    corpus_norms = np.linalg.norm(v.toarray(), axis=1)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = dot_products.flatten() / (query_norm * corpus_norms + 1e-10)  # small value to avoid division by zero\n",
    "\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9626011-7c49-4591-9122-7888d7eb154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "vectorizer.fit(corpus[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634190e-30ba-4c3e-8941-70c412b1c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100  # maximum size of each chunk\n",
    "\n",
    "sub_corpus_list = [df for _, df in corpus.groupby(np.arange(len(corpus)) // n)]\n",
    "sub_corpus_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec07d6c2-93a7-4c38-963e-ae28c65d4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_matches(query):\n",
    "    query_vector = vectorizer.transform([query['text']])\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = np.array([1,2,3,4,5,6,7,8,9,10,11]) #linear_kernel(query_vector, sub_corpus_matrix).flatten()\n",
    "    # Get top k corpus indices from this sub-corpus\n",
    "    top_k_indices = cosine_similarities.argsort()[-10:][::-1]  # Here, k=10\n",
    "    \n",
    "    local_results = []\n",
    "    for index in top_k_indices:\n",
    "        local_results.append({\n",
    "            'query_id': query['query-id'],\n",
    "            'corpus_id': sub_corpus_df.iloc[index]['_id']\n",
    "        })\n",
    "    return local_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee33297-187b-4646-abe7-cf1830592e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for sub_corpus_df in sub_corpus_list:\n",
    "    print(\"Transforming\")\n",
    "    sub_corpus_matrix = vectorizer.fit_transform(sub_corpus_df[\"text\"])\n",
    "    print(\"Parallelizing\")\n",
    "    # Use ProcessPoolExecutor to parallelize the inner loop\n",
    "    count = 0\n",
    "    for _, query in queries_test.iterrows():\n",
    "        top_k_indices = find_top_k_matches(query)\n",
    "        results.extend(top_k_indices)\n",
    "        if count%100==0:\n",
    "            print(count)\n",
    "        count +=1\n",
    "        \n",
    "        \n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('query_corpus_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dc001-ddc1-4097-997f-b3766ff25229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of documents and queries\n",
    "documents = corpus\n",
    "qs = queries_test \n",
    "\n",
    "# 1. Vectorize the documents using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "doc_vectors = vectorizer.fit_transform(documents[\"text\"]).toarray()\n",
    "\n",
    "# 2. Build Annoy Index\n",
    "f = doc_vectors.shape[1]  # Number of dimensions of the vector\n",
    "t = AnnoyIndex(f, 'angular')  # Use 'angular' for cosine similarity\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    t.add_item(i, doc_vectors[i])\n",
    "\n",
    "t.build(50)  # 50 trees. Increase if needed.\n",
    "\n",
    "# 3. Query the index\n",
    "results = {}\n",
    "\n",
    "for query in qs:\n",
    "    query_vector = vectorizer.transform([query]).toarray()[0]\n",
    "    top10_indices = t.get_nns_by_vector(query_vector, 10)  # Find the top 10 document indices\n",
    "    results[query] = [documents[i] for i in top10_indices]\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f9224b-d53b-4c24-87cf-e284fb82f59b",
   "metadata": {},
   "source": [
    "### TEST 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405da4a-01c7-4538-86da-186ca9d093a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_qrs_to_docs(qrs, dcs):\n",
    "    \n",
    "    # 1. Hashing: Convert documents to hashed vectors\n",
    "    start = time.time()\n",
    "    hash_vectorizer = HashingVectorizer(n_features=2**20, stop_words='english', norm=None)\n",
    "    hashed_docs = hash_vectorizer.transform(dcs[\"text\"])\n",
    "    end = time.time()\n",
    "    print(f\"Hashing documents took {end - start} seconds.\")\n",
    "    \n",
    "    # 2. Dimensionality Reduction\n",
    "    start = time.time()\n",
    "    transformer = SparseRandomProjection(n_components=100)  # Reducing to 100 dimensions\n",
    "    reduced_docs = transformer.fit_transform(hashed_docs).toarray()\n",
    "    end = time.time()\n",
    "    print(f\"Dimensionality reduction took {end - start} seconds.\")\n",
    "    \n",
    "    # Build Annoy Index\n",
    "    start = time.time()\n",
    "    f = reduced_docs.shape[1]\n",
    "    t = AnnoyIndex(f, 'angular')\n",
    "    for i, vector in enumerate(reduced_docs):\n",
    "        t.add_item(i, vector)\n",
    "    t.build(50)\n",
    "    end = time.time()\n",
    "    print(f\"Building Annoy index took {end - start} seconds.\")\n",
    "    \n",
    "    # Query the index\n",
    "    results = {}\n",
    "    start = time.time()\n",
    "    for index, row in qrs.iterrows():\n",
    "        query_text = row[\"text\"]\n",
    "        hashed_query = hash_vectorizer.transform([query_text])\n",
    "        reduced_query = transformer.transform(hashed_query).toarray()[0]\n",
    "        top10_indices = t.get_nns_by_vector(reduced_query, 10)\n",
    "        \n",
    "        # Storing the _id from documents\n",
    "        results[row[\"query-id\"]] = [dcs.iloc[i][\"corpus-id\"] for i in top10_indices]\n",
    "    end = time.time()\n",
    "    print(f\"Querying the index took {end - start} seconds.\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5a333-6158-4213-bb0b-dce51cef9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = map_qrs_to_docs(queries_train_subset, corpus)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28d2fe-9195-4d84-b18f-07add621bea5",
   "metadata": {},
   "source": [
    "### TEST 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c03e6d3-90e5-4bbe-b456-0489cb2d6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def build_inverted_index(docs):\n",
    "    index = defaultdict(set)\n",
    "    for doc_id, row in docs.iterrows():\n",
    "        # Remove punctuation and convert to lowercase\n",
    "        clean_text = remove_punctuation(row[\"text\"].lower())\n",
    "        for word in clean_text.split():\n",
    "            index[word].add(doc_id)\n",
    "    return index\n",
    "    \n",
    "def filter_docs(query, index):\n",
    "    relevant_doc_ids = set()\n",
    "    for word in query.split():\n",
    "        relevant_doc_ids.update(index.get(word, set()))\n",
    "    return relevant_doc_ids\n",
    "\n",
    "start = time.time()\n",
    "inverted_index = build_inverted_index(corpus)\n",
    "end = time.time()\n",
    "print(f\"Indexing took {end - start} seconds.\")\n",
    "inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98787daf-d411-44d0-8623-d3ac27fcc868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_from_index(inverted_index):\n",
    "    # Use a predefined list of stopwords (ENGLISH_STOP_WORDS from sklearn here)\n",
    "    for stopword in ENGLISH_STOP_WORDS:\n",
    "        if stopword in inverted_index:\n",
    "            del inverted_index[stopword]\n",
    "    return inverted_index\n",
    "inverted_index = remove_stopwords_from_index(inverted_index)\n",
    "inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a65b4-4c5d-4dcc-bef4-914de3810f48",
   "metadata": {},
   "source": [
    "## JUNK & OTHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ebce8-130c-4fd6-b711-01ebc8b70b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for idx, df in enumerate(sub_corpus_list):\n",
    "    # Transform the text in the dataframe using the vectorizer\n",
    "    tfidf_matrix = vectorizer.transform(df[\"text\"])\n",
    "    \n",
    "    # Convert the sparse matrix to a dense DataFrame\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    tfidf_df.to_csv(f'tfidf_matrix_{idx}.csv', index=False)\"\"\"\n",
    "\"\"\"\n",
    "for idx, df in enumerate(sub_corpus_list):\n",
    "    print(\"Transforming\")\n",
    "    tfidf_matrix = vectorizer.transform(df[\"text\"])\n",
    "    #print(\"framing\")\n",
    "    #tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    #print(\"storing\")\n",
    "    #tfidf_df.to_parquet(f'tfidf_matrix_{idx}.parquet')\n",
    "    for idx, q_text  in queries:\n",
    "        query_feature = tf.transform([query])\n",
    "        cosine_similarities = linear_kernel(query_feature,corpus_feature).flatten()\n",
    "        top_10 = np.argpartition(cosine_similarities,-5)[-5:]\"\"\"  \n",
    "# Placeholder for the results\n",
    "\n",
    "\"\"\"\n",
    "results = []\n",
    "# Assuming list_of_dfs is the list of your sub-corpuses created earlier\n",
    "for sub_corpus_df in sub_corpus_list:\n",
    "    print(\"Transform sub\")\n",
    "    sub_corpus_matrix = vectorizer.transform(sub_corpus_df[\"text\"])\n",
    "    \n",
    "    for _, query in queries.iterrows():\n",
    "        print(\"Treat query\")\n",
    "        query_vector = vectorizer.transform([query['text']])\n",
    "        \n",
    "        # Compute cosine similarities\n",
    "        cosine_similarities = linear_kernel(query_vector, sub_corpus_matrix).flatten()\n",
    "        \n",
    "        # Get top k corpus indices from this sub-corpus\n",
    "        top_k_indices = cosine_similarities.argsort()[-10:][::-1]  # Here, k=10\n",
    "        \n",
    "        for index in top_k_indices:\n",
    "            results.append({\n",
    "                'query_id': query['_id'],\n",
    "                'corpus_id': sub_corpus_df.iloc[index]['_id']\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('query_corpus_mapping.csv', index=False)\n",
    "\"\"\"\n",
    "def find_top_k_matches(query):\n",
    "    print(\"Transform query\")\n",
    "    query_vector = vectorizer.transform([query['text']])\n",
    "\n",
    "    print(\"Cosine Sim\")\n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = linear_kernel(query_vector, sub_corpus_matrix).flatten()\n",
    "\n",
    "    print(\"K vals\")\n",
    "    # Get top k corpus indices from this sub-corpus\n",
    "    top_k_indices = cosine_similarities.argsort()[-10:][::-1]  # Here, k=10\n",
    "    \n",
    "    local_results = []\n",
    "    for index in top_k_indices:\n",
    "        local_results.append({\n",
    "            'query_id': query['query_id'],\n",
    "            'corpus_id': sub_corpus_df.iloc[index]['corpus_id']\n",
    "        })\n",
    "    print(\"Finished query !\")\n",
    "    return local_results\n",
    "\"\"\"   \n",
    "def find_top_k_matches(query):\n",
    "    query_vector = vectorizer.transform([query['text']])\n",
    "    cosine_similarities = []\n",
    "\n",
    "    for idx in range(sub_corpus_matrix.shape[0]):\n",
    "        corpus_vector = sub_corpus_matrix[idx].toarray().flatten()\n",
    "        similarity = cosine_distance(query_vector.toarray().flatten(), corpus_vector)\n",
    "        cosine_similarities.append(similarity)\n",
    "\n",
    "    # Get top k corpus indices from this sub-corpus\n",
    "    top_k_indices = np.argsort(cosine_similarities)[-10:][::-1]  # Here, k=10\n",
    "    \n",
    "    local_results = []\n",
    "    for index in top_k_indices:\n",
    "        local_results.append({\n",
    "            'query_id': query['query_id'],\n",
    "            'corpus_id': sub_corpus_df.iloc[index]['corpus_id']\n",
    "        })\n",
    "    \n",
    "    return local_results\n",
    "\"\"\"\n",
    "results = []\n",
    "\n",
    "# Assuming list_of_dfs is the list of your sub-corpuses created earlier\n",
    "for sub_corpus_df in sub_corpus_list:\n",
    "    print(\"Transforming\")\n",
    "    sub_corpus_matrix = vectorizer.transform(sub_corpus_df[\"text\"])\n",
    "    print(\"Parallelizing\")\n",
    "    # Use ProcessPoolExecutor to parallelize the inner loop\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        results.extend(executor.map(find_top_k_matches, queries_test.iterrows()))\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('query_corpus_mapping.csv', index=False)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b17c5-58fe-49b1-9d8e-e2e1e38fde67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75435971-6d5e-48e5-8660-f05b51052077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
