{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb84fe67-47e4-4027-a1d4-9eec3906ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import downloader as api\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182ee2e-3277-4844-9174-b9729bd536c8",
   "metadata": {},
   "source": [
    "## Data Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6501acf-b8b7-43fc-91f6-6a0233f345a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus-id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000000</th>\n",
       "      <td>0</td>\n",
       "      <td>The presence of communication amid scientific ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966376</th>\n",
       "      <td>8</td>\n",
       "      <td>In June 1942, the United States Army Corps of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468831</th>\n",
       "      <td>12</td>\n",
       "      <td>Tutorial: Introduction to Restorative Justice....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000001</th>\n",
       "      <td>16</td>\n",
       "      <td>The approach is based on a theory of justice t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306952</th>\n",
       "      <td>23</td>\n",
       "      <td>Phloem is a conductive (or vascular) tissue fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950989</th>\n",
       "      <td>8841780</td>\n",
       "      <td>Wolves don't hide. They don't even live in cav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395590</th>\n",
       "      <td>8841787</td>\n",
       "      <td>The UNHCR Country Representative in Kenya. Str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93101</th>\n",
       "      <td>8841790</td>\n",
       "      <td>2. Describe the misery at Kakuma. 3. Compariso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669122</th>\n",
       "      <td>8841800</td>\n",
       "      <td>Following the death of his employer and mentor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593471</th>\n",
       "      <td>8841801</td>\n",
       "      <td>Presently, Puerto Rico holds the most titles f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1471406 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         corpus-id                                               text\n",
       "1000000          0  The presence of communication amid scientific ...\n",
       "966376           8  In June 1942, the United States Army Corps of ...\n",
       "468831          12  Tutorial: Introduction to Restorative Justice....\n",
       "1000001         16  The approach is based on a theory of justice t...\n",
       "306952          23  Phloem is a conductive (or vascular) tissue fo...\n",
       "...            ...                                                ...\n",
       "950989     8841780  Wolves don't hide. They don't even live in cav...\n",
       "395590     8841787  The UNHCR Country Representative in Kenya. Str...\n",
       "93101      8841790  2. Describe the misery at Kakuma. 3. Compariso...\n",
       "669122     8841800  Following the death of his employer and mentor...\n",
       "593471     8841801  Presently, Puerto Rico holds the most titles f...\n",
       "\n",
       "[1471406 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_json('data/corpus.jsonl', lines=True).sort_values(by=[\"_id\"]).rename(columns={\"_id\": \"corpus-id\"}).reset_index(drop=True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a717db0-efa2-4626-bf05-45ed2aad82b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Androgen receptor define</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1215</td>\n",
       "      <td>3 levels of government in canada and their res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1288</td>\n",
       "      <td>3/5 of 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1576</td>\n",
       "      <td>60x40 slab cost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2235</td>\n",
       "      <td>Bethel University was founded in what year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7432</th>\n",
       "      <td>1102335</td>\n",
       "      <td>why do people buy cars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7433</th>\n",
       "      <td>1102351</td>\n",
       "      <td>why do jefferson and stanton include these sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7434</th>\n",
       "      <td>1102390</td>\n",
       "      <td>why do children get aggressive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7435</th>\n",
       "      <td>1102393</td>\n",
       "      <td>why do celebrate st patrick's day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7436</th>\n",
       "      <td>1102400</td>\n",
       "      <td>why do bears hibernate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7437 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query-id                                               text\n",
       "0            2                           Androgen receptor define\n",
       "1         1215  3 levels of government in canada and their res...\n",
       "2         1288                                          3/5 of 60\n",
       "3         1576                                    60x40 slab cost\n",
       "4         2235         Bethel University was founded in what year\n",
       "...        ...                                                ...\n",
       "7432   1102335                             why do people buy cars\n",
       "7433   1102351  why do jefferson and stanton include these sim...\n",
       "7434   1102390                     why do children get aggressive\n",
       "7435   1102393                  why do celebrate st patrick's day\n",
       "7436   1102400                             why do bears hibernate\n",
       "\n",
       "[7437 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = pd.read_json(path_or_buf='data/queries.jsonl', lines=True).sort_values(by=[\"_id\"])\n",
    "queries['text'] = queries['text'].str.strip()\n",
    "queries = queries.drop(columns=[\"metadata\"]).rename(columns={\"_id\": \"query-id\"})\n",
    "queries\n",
    "\n",
    "df_test = pd.read_csv(\"data/task1_test.tsv\", sep=\"\\t\")\n",
    "queries_test = pd.merge(queries, df_test, left_on='query-id', right_on='query-id', how='inner').drop(columns=[\"id\"])\n",
    "\n",
    "##Free queries\n",
    "queries = None\n",
    "\n",
    "queries_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7684f19-5008-441e-92d9-8b40b82fa697",
   "metadata": {},
   "source": [
    "### Importing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61ec46f3-32ca-4f2e-bd2f-f8865fb50e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = KeyedVectors.load('data/glove.model.d2v')\n",
    "except:\n",
    "    print(\"404, Now Fetching Model ...\")\n",
    "    model = api.load(\"glove-wiki-gigaword-50\")\n",
    "    model.save('data/glove.model.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a3f68-6889-46e7-a29e-3a6b4b73ca6e",
   "metadata": {},
   "source": [
    "### Prepare text processing constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fb3c8bc-0585-4807-b4d2-8f161e0fdef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEMMER = PorterStemmer()\n",
    "NON_ASCII_PATTERN = re.compile(r'\\\\u[0-9a-fA-F]{4}')\n",
    "STOPWORDS_SET = set(stopwords.words(\"english\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d943715b-bb22-4a31-9f7a-47f07e4ca41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the given text by performing several operations:\n",
    "    1. Converts the text to lowercase.\n",
    "    2. Removes non-ASCII characters.\n",
    "    3. Replaces punctuation with spaces.\n",
    "    4. Removes digits.\n",
    "    5. Tokenizes the text using NLTK's word_tokenize.\n",
    "    6. Removes stopwords and stems the words using PorterStemmer.\n",
    "    7. Filters out words that are not in the model vocabulary.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A list of preprocessed and tokenized words.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = NON_ASCII_PATTERN.sub('', text)\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    \n",
    "    words = nltk.word_tokenize(text)\n",
    "    stopwords_ = stopwords.words('english')\n",
    "    words = [STEMMER.stem(word) for word in words if word not in STOPWORDS_SET and word in model]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56711ff3-28ef-44de-9a21-7a8dd799251d",
   "metadata": {},
   "source": [
    "##  TF-IDF Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6169b048-bf5d-4f73-bd58-cb7c28eec053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_tfidf_dataframe_sparse(documents, vocabulary):\n",
    "    \"\"\"\n",
    "    Generates a term frequency (TF) matrix for the given documents and vocabulary.\n",
    "\n",
    "    Args:\n",
    "    - documents (list of list of str): The preprocessed documents represented as lists of words.\n",
    "    - vocabulary (list of str): The unique words to be considered from all documents.\n",
    "\n",
    "    Returns:\n",
    "    - lil_matrix: A sparse matrix representation of the term frequencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a sparse matrix to hold the term frequencies\n",
    "    tf_matrix = lil_matrix((len(documents), len(vocabulary)), dtype=int)\n",
    "\n",
    "    # Map each word in the vocabulary to its column index for faster lookup\n",
    "    vocab_index_map = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        for word in doc:\n",
    "            if word in vocab_index_map:\n",
    "                tf_matrix[i, vocab_index_map[word]] += 1\n",
    "\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9195aae4-7e3c-4efc-b9e1-b20c5e65cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(corpus_text):\n",
    "    \"\"\"\n",
    "    Computes the Term Frequency-Inverse Document Frequency (TF-IDF) matrix for the given corpus.\n",
    "\n",
    "    Args:\n",
    "    - corpus_text (iterable): The input corpus where each item is a raw text document.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the following:\n",
    "        1. documents (list of list of str): Preprocessed documents.\n",
    "        2. tfidf_matrix (csr_matrix): The computed TF-IDF matrix.\n",
    "        3. vocabulary (list of str): The vocabulary extracted from the corpus.\n",
    "        4. idf (numpy array): The computed inverse document frequencies for each word in the vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Process docs ...\")\n",
    "    documents = corpus_text.progress_apply(lambda x: preprocess_text(x))\n",
    "    \n",
    "    print(\"Create vocab ...\")\n",
    "    vocabulary = list(set(word for doc in documents for word in doc))\n",
    "    vocabulary.sort()\n",
    "\n",
    "    print(\"Compute tf ...\")\n",
    "    tf_matrix = populate_tfidf_dataframe_sparse(documents, vocabulary)\n",
    "\n",
    "    print(\"Compute idf ...\")\n",
    "    doc_count = len(documents)\n",
    "    df = (tf_matrix > 0).sum(axis=0)\n",
    "    idf = np.log((doc_count + 0.5) / (df + 0.5))\n",
    "    \n",
    "    print(\"Compute tf-idf ...\")\n",
    "    tf_matrix = tf_matrix.tocsr()\n",
    "    tf_matrix = tf_matrix.multiply(1 / tf_matrix.sum(axis=1))\n",
    "    tfidf_matrix = tf_matrix.multiply(idf)\n",
    "\n",
    "    print(\"Done!\")\n",
    "    return documents, tfidf_matrix, vocabulary, idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d8219-4390-4102-9099-062b1c54504c",
   "metadata": {},
   "source": [
    "### TF-IDF Corpus Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c69b034b-3c8d-4170-b60d-b33e6cd0fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_process_corpus():\n",
    "    TFIDF_FOLDER = \"data/\"\n",
    "    FILE_NAME = \"submission\"\n",
    "    try:\n",
    "        tf_idf = pd.read_pickle(f'{TFIDF_FOLDER}tfidf-{FILE_NAME}.pkl')\n",
    "        idf = pd.read_pickle(f'{IDF_FOLDER}idf-{FILE_NAME}.pkl')\n",
    "        vocabulary = pd.read_pickle(f'{VOCABULARY_FOLDER}vocabulary-{FILE_NAME}.pkl')\n",
    "        documents = pd.read_pickle(f'{DOCUMENT_FOLDER}document-{FILE_NAME}.pkl')\n",
    "        return documents, tf_idf, vocabulary, idf\n",
    "    except:\n",
    "        print(\"404, creating required metadata ...\")\n",
    "        documents, tf_idf, vocabulary, idf = tfidf(corpus[\"text\"])\n",
    "        \n",
    "        with open(f'{TFIDF_FOLDER}tfidf-{FILE_NAME}.pkl', 'wb') as f:\n",
    "            pickle.dump(tf_idf, f)\n",
    "\n",
    "        with open(f'{IDF_FOLDER}idf-{FILE_NAME}.pkl', 'wb') as f:\n",
    "            pickle.dump(idf, f)\n",
    "    \n",
    "        with open(f'{VOCABULARY_FOLDER}vocabulary-{FILE_NAME}.pkl', 'wb') as f:\n",
    "            pickle.dump(vocabulary, f)\n",
    "            \n",
    "        with open(f'{DOCUMENT_FOLDER}document-{FILE_NAME}.pkl', 'wb') as f:\n",
    "            pickle.dump(documents, f)\n",
    "\n",
    "        return documents, tf_idf, vocabulary, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a6c41-89ca-4c1b-be9a-fceb9be9cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_process_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188dd49-843b-4cf9-80ea-4db4c43a00d0",
   "metadata": {},
   "source": [
    "### TF-IDF Query Processing & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a61123f4-3b40-486c-9da3-5bfa1e717171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorize_queries(queries_df, vocabulary, idf):\n",
    "    \"\"\"Convert each query in the DataFrame into its TF-IDF vector.\"\"\"\n",
    "    \n",
    "    print(\"Process queries ...\")\n",
    "    # Preprocess all queries\n",
    "    queries_df['processed'] = queries_df['text'].apply(preprocess_query)\n",
    "\n",
    "    print(\"Initialize sparse matrix ...\")\n",
    "    num_queries = len(queries_df)\n",
    "    num_terms = len(vocabulary)\n",
    "    \n",
    "    # Using a dictionary for term index lookup\n",
    "    vocab_dict = {term: index for index, term in enumerate(vocabulary)}\n",
    "    tf_matrix = lil_matrix((num_queries, num_terms))\n",
    "\n",
    "    print(\"Compute  tf ...\")\n",
    "    # Populate the sparse matrix\n",
    "    for idx, row in queries_df.iterrows():\n",
    "        for term in row['processed']:\n",
    "            if term in vocab_dict:\n",
    "                tf_matrix[idx, vocab_dict[term]] += 1\n",
    "\n",
    "    print(\"Multiply by idf ...\")\n",
    "    # Convert to CSR format for efficient multiplication and transform TFs to TF-IDF\n",
    "    tfidf_matrix = (tf_matrix.tocsr()).multiply(idf)\n",
    "\n",
    "    print(\"Done !\")\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efc26491-b76f-4bd0-8bbf-21be5b0b4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_indices_sparse(matrix: csr_matrix, k: int):\n",
    "    \"\"\"Get top k indices for each row of a sparse matrix.\"\"\"\n",
    "    \n",
    "    # Placeholder list for top k indices for each row\n",
    "    top_indices = []\n",
    "    \n",
    "    # Iterate over each row\n",
    "    print('Iterate over each row ...')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        row_data = matrix.data[matrix.indptr[i]:matrix.indptr[i+1]]\n",
    "        row_indices = matrix.indices[matrix.indptr[i]:matrix.indptr[i+1]]\n",
    "        \n",
    "        # If the row has less than k values, take them all\n",
    "        if len(row_data) < k:\n",
    "            top_indices.append(row_indices)\n",
    "        else:\n",
    "            # Sort the row data and get top k indices\n",
    "            sorted_indices = np.argsort(-row_data)\n",
    "            top_indices.append(row_indices[sorted_indices[:k]])\n",
    "    \n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c67d459-c5be-4d40-a726-143970e9edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_predict_documents(tfidf_matrix_normalized, query_vectors, k):\n",
    "    \"\"\"Process multiple queries and return ranked document indices for each query.\"\"\"\n",
    "    \n",
    "    # Compute cosine similarities using matrix operations\n",
    "    print(\"Compute cosine similarities ...\")\n",
    "    similarity_matrix = cosine_similarity(query_vectors, tfidf_matrix_normalized, dense_output=False)\n",
    "    \n",
    "    # Get document indices ranked by relevance for each query\n",
    "    print(\"Rank documents ...\")\n",
    "    # print(similarity_matrix.shape)\n",
    "    # ranked_doc_indices = np.argsort(-similarity_matrix)[:, :k]\n",
    "    ranked_doc_indices = top_k_indices_sparse(similarity_matrix, k)\n",
    "    \n",
    "    return ranked_doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "716fffe9-ddfe-4228-ba45-999b4f693b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_ids_ranking(corpus, queries, prediction):\n",
    "    # Map the prediction rows to the corresponding 'corpus-id' values from the corpus\n",
    "    mapped_results = [corpus.iloc[row]['corpus-id'].values.tolist() for row in prediction]\n",
    "\n",
    "    # Create a DataFrame with 'id', 'corpus-id', and 'score' columns\n",
    "    df = pd.DataFrame({\n",
    "        'id': queries['query-id'].iloc[:len(mapped_results)],\n",
    "        'corpus-id': mapped_results,\n",
    "        'score': [-1 for _ in range(len(mapped_results))]\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b82fc-dab5-476a-90eb-c0ce2f6fff35",
   "metadata": {},
   "source": [
    "### Deep Embedder Corpus Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7e3632d-3fab-4b1d-898a-f2c1fa2373e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embedder():\n",
    "    try:\n",
    "        with open('DeepEmbedder.pkl', 'rb') as f:\n",
    "            deep_embedder = pickle.load(f)\n",
    "        return deep_embedder\n",
    "    except:\n",
    "        print('404, Fetching DeepEmbedder')\n",
    "        deep_embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        with open(f'DeepEmbedder.pkl', 'wb') as f:\n",
    "            pickle.dump(deep_embedder, f)\n",
    "        return deep_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf937e7d-bd09-4dae-9d62-68804e3ad2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEP_EMBEDDER = load_pretrained_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41a0d7a1-7916-4e06-a55d-e2362c462c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_embedder_process_corpus():\n",
    "    try:\n",
    "        with open('EmbededCorpus.pkl', 'rb') as f:\n",
    "            embedded_corpus = pickle.load(f)\n",
    "        return embedded_corpus\n",
    "    except:\n",
    "        print('404, Computing Embeded Corpus ...')\n",
    "        embedded_corpus = DEEP_EMBEDDER.encode(sentences=corpus[\"text\"].tolist(), \n",
    "                                       batch_size= 128, # TO BE CHANGED\n",
    "                                       show_progress_bar=True, \n",
    "                                       device='cpu', # TO BE CHANGED -- 'cpu', 'cuda', automatic if None\n",
    "                                       )\n",
    "        \n",
    "        with open(f'EmbededCorpus.pkl', 'wb') as f:\n",
    "            pickle.dump(embedded_corpus, f)\n",
    "        return embedded_corpus  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a796a-5fa0-4176-9460-92b4931a8f50",
   "metadata": {},
   "source": [
    "### Deep Embedder Query Processing & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8237bc28-6a5e-4ec9-a401-1b76c66a8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_vectorize_queries(queries):\n",
    "    return deep_embedder.encode(queries.text.tolist(),\n",
    "                                          batch_size= 128, # TO BE CHANGED \n",
    "                                          show_progress_bar=True, \n",
    "                                          device='cpu', # TO BE CHANGED -- 'cpu', 'cuda', automatic if None\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "844cda11-52d8-4d6b-8ec5-653c2179a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_predict_documents(top_large_k, vectorized_queries,  vectorized_corpus):\n",
    "    \n",
    "    # 2D Array for storing indices to relevant documents\n",
    "    # Shape (Number of queries, k)\n",
    "    top_10 = np.zeros((vectorized_queries.shape[0], 10))\n",
    "    \n",
    "    # Iterate through each query embedding\n",
    "    for idx, vector_query in enumerate(vectorized_queries):\n",
    "    \n",
    "        # Index the embedding of relevant candidates\n",
    "        # Shape of sentence_feature: (large_k, 384)\n",
    "        sentence_feature = vectorized_corpus[top_large_k[idx]]\n",
    "    \n",
    "        # Dot product (numerator of cosine similarity), similar to linear_kernel\n",
    "        similarity = sentence_feature @ vector_query\n",
    "    \n",
    "        # Get indices of top-k highest similarities\n",
    "        top_10[idx] = np.argsort(similarity)[-10:] \n",
    "    return top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45af34e5-7b67-4970-9a2b-1d1461467535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404, creating required metadata ...\n",
      "Process docs ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                                | 11546/1471406 [00:11<23:16, 1045.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ym/9zzn1f9j7c7dnh381g3rrm_m0000gn/T/ipykernel_16329/2215084635.py\u001b[0m in \u001b[0;36mtfidf_process_corpus\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{TFIDF_FOLDER}tfidf-{FILE_NAME}.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{IDF_FOLDER}idf-{FILE_NAME}.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/tfidf-submission.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ym/9zzn1f9j7c7dnh381g3rrm_m0000gn/T/ipykernel_16329/2215084635.py\u001b[0m in \u001b[0;36mtfidf_process_corpus\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"404, creating required metadata ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{TFIDF_FOLDER}tfidf-{FILE_NAME}.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ym/9zzn1f9j7c7dnh381g3rrm_m0000gn/T/ipykernel_16329/3837129066.py\u001b[0m in \u001b[0;36mtfidf\u001b[0;34m(corpus_text)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Process docs ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Create vocab ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4628\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4629\u001b[0m         \"\"\"\n\u001b[0;32m-> 4630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4632\u001b[0m     def _reduce(\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1077\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ym/9zzn1f9j7c7dnh381g3rrm_m0000gn/T/ipykernel_16329/3837129066.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Process docs ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Create vocab ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ym/9zzn1f9j7c7dnh381g3rrm_m0000gn/T/ipykernel_16329/2771991291.py\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mstopwords_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSTEMMER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTOPWORDS_SET\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     return [\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     ]\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     return [\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     ]\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/nltk/tokenize/destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENDING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## TF-IDF PREDICTION \n",
    "k = 1000\n",
    "documents, tfidf, vocabulary, idf = tfidf_process_corpus()\n",
    "tfidf_vectors = vectorize_queries(queries_test, vocabulary, idf)\n",
    "prediction = predict_documents(tfidf, vectors, tfidf_vectors)\n",
    "map_ = predictions_to_ids_ranking(corpus, queries_test, prediction)\n",
    "\n",
    "## DEEP EMBEDDING PREDICTION\n",
    "VECTORIZED_CORPUS = deep_embedder_process_corpus()\n",
    "top_large_k = map_[\"corpus-id\"].apply(lambda x: list(x))\n",
    "deep_vectors =  deep_vectorize_queries(queries)\n",
    "\n",
    "top10 = deep_predict_documents(top_large_k, deep_vectors,  VECTORIZED_CORPUS)\n",
    "top10\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c05a8-be01-41a6-b7b8-d0e903d49ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
