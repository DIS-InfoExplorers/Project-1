{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in c:\\users\\balanton\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.32.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from sentence_transformers) (2.1.0+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.17.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from nltk->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from torchvision->sentence_transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\balanton\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T21:14:56.294384Z",
     "start_time": "2023-10-21T21:14:46.403344200Z"
    }
   },
   "id": "1174017861fec0de"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb84fe67-47e4-4027-a1d4-9eec3906ff8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:07.003108300Z",
     "start_time": "2023-10-21T21:14:56.296389100Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import downloader as api\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182ee2e-3277-4844-9174-b9729bd536c8",
   "metadata": {},
   "source": [
    "## Data Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6501acf-b8b7-43fc-91f6-6a0233f345a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:18.291963100Z",
     "start_time": "2023-10-21T21:15:07.010138500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         corpus-id                                               text\n0                0  The presence of communication amid scientific ...\n1                8  In June 1942, the United States Army Corps of ...\n2               12  Tutorial: Introduction to Restorative Justice....\n3               16  The approach is based on a theory of justice t...\n4               23  Phloem is a conductive (or vascular) tissue fo...\n...            ...                                                ...\n1471401    8841780  Wolves don't hide. They don't even live in cav...\n1471402    8841787  The UNHCR Country Representative in Kenya. Str...\n1471403    8841790  2. Describe the misery at Kakuma. 3. Compariso...\n1471404    8841800  Following the death of his employer and mentor...\n1471405    8841801  Presently, Puerto Rico holds the most titles f...\n\n[1471406 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>corpus-id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>The presence of communication amid scientific ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>In June 1942, the United States Army Corps of ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12</td>\n      <td>Tutorial: Introduction to Restorative Justice....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16</td>\n      <td>The approach is based on a theory of justice t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23</td>\n      <td>Phloem is a conductive (or vascular) tissue fo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1471401</th>\n      <td>8841780</td>\n      <td>Wolves don't hide. They don't even live in cav...</td>\n    </tr>\n    <tr>\n      <th>1471402</th>\n      <td>8841787</td>\n      <td>The UNHCR Country Representative in Kenya. Str...</td>\n    </tr>\n    <tr>\n      <th>1471403</th>\n      <td>8841790</td>\n      <td>2. Describe the misery at Kakuma. 3. Compariso...</td>\n    </tr>\n    <tr>\n      <th>1471404</th>\n      <td>8841800</td>\n      <td>Following the death of his employer and mentor...</td>\n    </tr>\n    <tr>\n      <th>1471405</th>\n      <td>8841801</td>\n      <td>Presently, Puerto Rico holds the most titles f...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1471406 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_json('data/corpus.jsonl', lines=True).sort_values(by=[\"_id\"]).rename(\n",
    "    columns={\"_id\": \"corpus-id\"}).reset_index(drop=True)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "        query-id                                               text\n0        1185869  )what was the immediate impact of the success ...\n1        1185868  _________ justice is designed to repair the ha...\n2         597651                          what color is amber urine\n3         403613  is autoimmune hepatitis a bile acid synthesis ...\n4        1183785                                     elegxo meaning\n...          ...                                                ...\n509957    147073  difference between discrete and process manufa...\n509958    243761                 how long did abraham lincoln serve\n509959    162662       does adult acne rosacea give you blepharitis\n509960    247194                       how long do you bake muffins\n509961    195199                                     glioma meaning\n\n[509962 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query-id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1185869</td>\n      <td>)what was the immediate impact of the success ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1185868</td>\n      <td>_________ justice is designed to repair the ha...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>597651</td>\n      <td>what color is amber urine</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>403613</td>\n      <td>is autoimmune hepatitis a bile acid synthesis ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1183785</td>\n      <td>elegxo meaning</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>509957</th>\n      <td>147073</td>\n      <td>difference between discrete and process manufa...</td>\n    </tr>\n    <tr>\n      <th>509958</th>\n      <td>243761</td>\n      <td>how long did abraham lincoln serve</td>\n    </tr>\n    <tr>\n      <th>509959</th>\n      <td>162662</td>\n      <td>does adult acne rosacea give you blepharitis</td>\n    </tr>\n    <tr>\n      <th>509960</th>\n      <td>247194</td>\n      <td>how long do you bake muffins</td>\n    </tr>\n    <tr>\n      <th>509961</th>\n      <td>195199</td>\n      <td>glioma meaning</td>\n    </tr>\n  </tbody>\n</table>\n<p>509962 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = pd.read_json(path_or_buf='data/queries.jsonl', lines=True)\n",
    "queries['text'] = queries['text'].str.strip()\n",
    "queries = queries.drop(columns=[\"metadata\"]).rename(columns={\"_id\": \"query-id\"})\n",
    "queries"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:21.940946200Z",
     "start_time": "2023-10-21T21:15:18.289945700Z"
    }
   },
   "id": "86264e7dd0e733dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mapping queries ids to queries texts for task 1 and task 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaf3d6fd0cb95fb"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a717db0-efa2-4626-bf05-45ed2aad82b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:22.290452400Z",
     "start_time": "2023-10-21T21:15:21.941964600Z"
    }
   },
   "outputs": [],
   "source": [
    "df_task_1 = pd.read_csv(\"data/task1_test.tsv\", sep=\"\\t\")\n",
    "queries_test = pd.merge(queries, df_task_1, left_on='query-id', right_on='query-id', how='inner').drop(columns=[\"id\"])\n",
    "\n",
    "df_task_2 = pd.read_csv(\"data/task2_test.tsv\", sep=\"\\t\")\n",
    "df_task_2 = pd.merge(df_task_2, queries, left_on='query-id', right_on='query-id')\n",
    "df_task_2 = df_task_2.drop(columns=['query-id'])\n",
    "df_task_2['corpus-id'] = df_task_2['corpus-id'].str.replace(\"[\", '')\n",
    "df_task_2['corpus-id'] = df_task_2['corpus-id'].str.replace(\"]\", '')\n",
    "df_task_2['corpus-id'] = df_task_2['corpus-id'].str.split(\", \")\n",
    "\n",
    "##Free queries\n",
    "queries = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7684f19-5008-441e-92d9-8b40b82fa697",
   "metadata": {},
   "source": [
    "### Importing GloVe Model\n",
    "Loading pre-trained word-vectors from [gensim-data](https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html).\n",
    "We are using the [glove-wiki-gigaword-50](https://nlp.stanford.edu/projects/glove/) model ([PDF](https://nlp.stanford.edu/pubs/glove.pdf)).\n",
    "[Models available to download in gensim](https://github.com/piskvorky/gensim-data#models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ec46f3-32ca-4f2e-bd2f-f8865fb50e84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:22.708202900Z",
     "start_time": "2023-10-21T21:15:22.274306Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model = KeyedVectors.load('data/glove.model.d2v')\n",
    "except:\n",
    "    print(\"404, Now Fetching Model ...\")\n",
    "    model = api.load(\"glove-wiki-gigaword-50\")\n",
    "    model.save('data/glove.model.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare text processing constants\n",
    "For text processing, we will use the following:\n",
    "- [PorterStemmer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter) from NLTK for stemming.\n",
    "- [stopwords](https://www.nltk.org/api/nltk.corpus.html#module-nltk.corpus.stopwords) from NLTK for removing stopwords.\n",
    "- [string.punctuation](https://docs.python.org/3/library/string.html#string.punctuation) from Python for removing punctuation.\n",
    "- [string.digits](https://docs.python.org/3/library/string.html#string.digits) from Python for removing digits.\n",
    "- [re](https://docs.python.org/3/library/re.html) from Python for removing non-ASCII characters.\n",
    "- [nltk.word_tokenize](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize) from NLTK for tokenizing text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a08752bc249ddea"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb3c8bc-0585-4807-b4d2-8f161e0fdef4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:22.735684400Z",
     "start_time": "2023-10-21T21:15:22.702949300Z"
    }
   },
   "outputs": [],
   "source": [
    "STEMMER = PorterStemmer()\n",
    "NON_ASCII_PATTERN = re.compile(r'\\\\u[0-9a-fA-F]{4}')\n",
    "STOPWORDS_SET = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d943715b-bb22-4a31-9f7a-47f07e4ca41d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:22.736674400Z",
     "start_time": "2023-10-21T21:15:22.721085600Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the given text by performing several operations:\n",
    "    1. Converts the text to lowercase.\n",
    "    2. Removes non-ASCII characters.\n",
    "    3. Replaces punctuation with spaces.\n",
    "    4. Removes digits.\n",
    "    5. Tokenizes the text using NLTK's word_tokenize.\n",
    "    6. Removes stopwords and stems the words using PorterStemmer.\n",
    "    7. Filters out words that are not in the model vocabulary.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    - words: A list of preprocessed and tokenized words.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = NON_ASCII_PATTERN.sub('', text)\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [STEMMER.stem(word) for word in words if word not in STOPWORDS_SET and word in model]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56711ff3-28ef-44de-9a21-7a8dd799251d",
   "metadata": {},
   "source": [
    "##  TF-IDF Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6169b048-bf5d-4f73-bd58-cb7c28eec053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:22.789965800Z",
     "start_time": "2023-10-21T21:15:22.736674400Z"
    }
   },
   "outputs": [],
   "source": [
    "def populate_tfidf_dataframe(documents, vocabulary):\n",
    "    \"\"\"\n",
    "    Generates a term frequency (TF) matrix for the given documents and vocabulary.\n",
    "\n",
    "    Args:\n",
    "    - documents: The preprocessed documents represented as lists of words.\n",
    "    - vocabulary: The unique words to be considered from all documents.\n",
    "\n",
    "    Returns:\n",
    "    - tf_matrix: A sparse matrix representation of the term frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a sparse matrix to hold the term frequencies\n",
    "    tf_matrix = lil_matrix((len(documents), len(vocabulary)), dtype=int)\n",
    "\n",
    "    # Map each word in the vocabulary to its column index for faster lookup\n",
    "    vocab_index_map = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        for word in doc:\n",
    "            if word in vocab_index_map:\n",
    "                tf_matrix[i, vocab_index_map[word]] += 1\n",
    "\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9195aae4-7e3c-4efc-b9e1-b20c5e65cf24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:22.790963Z",
     "start_time": "2023-10-21T21:15:22.756694500Z"
    }
   },
   "outputs": [],
   "source": [
    "def tfidf(corpus_text):\n",
    "    \"\"\"\n",
    "    Computes the Term Frequency-Inverse Document Frequency (TF-IDF) matrix for the given corpus.\n",
    "\n",
    "    Args:\n",
    "    - corpus_text: The input corpus where each item is a raw text document.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the following:\n",
    "        1. documents: Preprocessed documents.\n",
    "        2. tfidf_matrix: The computed TF-IDF matrix.\n",
    "        3. vocabulary: The vocabulary extracted from the corpus.\n",
    "        4. idf: The computed inverse document frequencies for each word in the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Process docs ...\")\n",
    "    documents = corpus_text.progress_apply(lambda x: preprocess_text(x))\n",
    "\n",
    "    print(\"Create vocab ...\")\n",
    "    vocabulary = list(set(word for doc in documents for word in doc))\n",
    "    vocabulary.sort()\n",
    "\n",
    "    print(\"Compute tf ...\")\n",
    "    tf_matrix = populate_tfidf_dataframe(documents, vocabulary)\n",
    "\n",
    "    print(\"Compute idf ...\")\n",
    "    doc_count = len(documents)\n",
    "    df = (tf_matrix > 0).sum(axis=0)\n",
    "    idf = np.log((doc_count + 0.5) / (df + 0.5))\n",
    "\n",
    "    print(\"Compute tf-idf ...\")\n",
    "    tf_matrix = tf_matrix.tocsr()\n",
    "    tf_matrix = tf_matrix.multiply(1 / tf_matrix.sum(axis=1))\n",
    "    tfidf_matrix = tf_matrix.multiply(idf)\n",
    "\n",
    "    print(\"Done!\")\n",
    "    return documents, tfidf_matrix, vocabulary, idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d8219-4390-4102-9099-062b1c54504c",
   "metadata": {},
   "source": [
    "### TF-IDF Corpus Processing\n",
    "To avoid re-computing the TF-IDF matrix every time, we will save the computed matrix to disk using [pickle](https://docs.python.org/3/library/pickle.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c69b034b-3c8d-4170-b60d-b33e6cd0fa64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:22.791966900Z",
     "start_time": "2023-10-21T21:15:22.773482700Z"
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_process_corpus():\n",
    "    \"\"\"\n",
    "    Load or compute TF-IDF vectors and related metadata for a given corpus.\n",
    "\n",
    "    Args:\n",
    "        corpus (pandas.DataFrame): DataFrame containing 'text' column with raw text data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing four elements -\n",
    "            1. documents: Processed documents from the corpus.\n",
    "            2. tf_idf: TF-IDF matrix for the documents.\n",
    "            3. vocabulary: Vocabulary list used for TF-IDF vectorization.\n",
    "            4. idf: Inverse Document Frequencies (IDF) for the terms in the vocabulary.\n",
    "    \"\"\"\n",
    "    DATA_FOLDER = \"data/\"\n",
    "    FILE_NAME = \"submission\"\n",
    "    try:\n",
    "        tf_idf = pd.read_pickle(f'{DATA_FOLDER}tfidf-{FILE_NAME}.pkl')\n",
    "        idf = pd.read_pickle(f'{DATA_FOLDER}idf-{FILE_NAME}.pkl')\n",
    "        vocabulary = pd.read_pickle(f'{DATA_FOLDER}vocabulary-{FILE_NAME}.pkl')\n",
    "        documents = pd.read_pickle(f'{DATA_FOLDER}document-{FILE_NAME}.pkl')\n",
    "        return documents, tf_idf, vocabulary, idf\n",
    "    except:\n",
    "        print(\"404, creating required metadata ...\")\n",
    "        documents, tf_idf, vocabulary, idf = tfidf(corpus[\"text\"])\n",
    "\n",
    "        with open(f'{DATA_FOLDER}tfidf-{FILE_NAME}.pkl', 'wb') as f:\n",
    "            pickle.dump(tf_idf, f)\n",
    "\n",
    "        with open(f'{DATA_FOLDER}idf-{FILE_NAME}.pkl', 'wb') as f:\n",
    "            pickle.dump(idf, f)\n",
    "\n",
    "        with open(f'{DATA_FOLDER}vocabulary-{FILE_NAME}.pkl', 'wb') as f:\n",
    "            pickle.dump(vocabulary, f)\n",
    "\n",
    "        with open(f'{DATA_FOLDER}document-{FILE_NAME}.pkl', 'wb') as f:\n",
    "            pickle.dump(documents, f)\n",
    "\n",
    "        return documents, tf_idf, vocabulary, idf"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Execution cell for TF-IDF corpus processing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b32f9e6511a1771"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e68a6c41-89ca-4c1b-be9a-fceb9be9cc32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:41.880165300Z",
     "start_time": "2023-10-21T21:15:22.790963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9.14 s\n",
      "Wall time: 19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0          [presenc, commun, amid, scientif, mind, equal,...\n 1          [june, unit, state, armi, corp, manhattan, pro...\n 2          [tutori, introduct, restor, justic, restor, ju...\n 3          [approach, base, theori, justic, consid, crime...\n 4          [phloem, conduct, vascular, tissu, found, plan...\n                                  ...                        \n 1471401    [wolv, hide, even, live, cave, live, open, for...\n 1471402    [unhcr, countri, repres, kenya, street, addres...\n 1471403    [describ, miseri, kakuma, comparison, popul, k...\n 1471404    [follow, death, employ, mentor, bumpi, johnson...\n 1471405    [present, puerto, rico, hold, titl, miss, univ...\n Name: text, Length: 1471406, dtype: object,\n <1471406x135442 sparse matrix of type '<class 'numpy.float64'>'\n \twith 34858541 stored elements in COOrdinate format>,\n ['a',\n  'aa',\n  'aaa',\n  'aaaa',\n  'aaaaa',\n  'aaah',\n  'aaahh',\n  'aab',\n  'aaba',\n  'aabb',\n  'aac',\n  'aacc',\n  'aach',\n  'aachen',\n  'aacm',\n  'aacn',\n  'aacr',\n  'aacsb',\n  'aacta',\n  'aad',\n  'aadhi',\n  'aadi',\n  'aadt',\n  'aadu',\n  'aadvantag',\n  'aae',\n  'aaf',\n  'aafc',\n  'aaftc',\n  'aag',\n  'aagaard',\n  'aah',\n  'aaha',\n  'aai',\n  'aaii',\n  'aaj',\n  'aaja',\n  'aak',\n  'aakash',\n  'aaker',\n  'aal',\n  'aalborg',\n  'aalen',\n  'aaliyah',\n  'aalsmeer',\n  'aalto',\n  'aam',\n  'aama',\n  'aamc',\n  'aami',\n  'aamir',\n  'aamodt',\n  'aan',\n  'aana',\n  'aand',\n  'aang',\n  'aani',\n  'aann',\n  'aanr',\n  'aao',\n  'aap',\n  'aapa',\n  'aapb',\n  'aapc',\n  'aapg',\n  'aapk',\n  'aapl',\n  'aapp',\n  'aapt',\n  'aar',\n  'aarabi',\n  'aarau',\n  'aard',\n  'aardman',\n  'aardsma',\n  'aardvark',\n  'aardwolf',\n  'aargau',\n  'aarhu',\n  'aari',\n  'aarn',\n  'aaron',\n  'aaronson',\n  'aarp',\n  'aart',\n  'aarthi',\n  'aarti',\n  'aaru',\n  'aarushi',\n  'aas',\n  'aasa',\n  'aascu',\n  'aasen',\n  'aashiqui',\n  'aashish',\n  'aashto',\n  'aasm',\n  'aastha',\n  'aasu',\n  'aat',\n  'aata',\n  'aati',\n  'aau',\n  'aaup',\n  'aav',\n  'ab',\n  'aba',\n  'abab',\n  'ababa',\n  'abac',\n  'abaca',\n  'abacavir',\n  'abacha',\n  'abaci',\n  'aback',\n  'abaco',\n  'abacu',\n  'abad',\n  'abada',\n  'abadan',\n  'abaddon',\n  'abadi',\n  'abadia',\n  'abaft',\n  'abag',\n  'abagnal',\n  'abah',\n  'abair',\n  'abajo',\n  'abakanowicz',\n  'abala',\n  'abalon',\n  'abam',\n  'aban',\n  'abana',\n  'abandon',\n  'abang',\n  'abani',\n  'abano',\n  'abap',\n  'abar',\n  'abarkuh',\n  'abarth',\n  'abas',\n  'abash',\n  'abasolo',\n  'abassi',\n  'abat',\n  'abattoir',\n  'abawi',\n  'abaxi',\n  'abay',\n  'abaya',\n  'abb',\n  'abba',\n  'abbad',\n  'abbadi',\n  'abbasi',\n  'abbasid',\n  'abbassi',\n  'abbay',\n  'abbess',\n  'abbevil',\n  'abbey',\n  'abbeyleix',\n  'abbi',\n  'abbington',\n  'abbitt',\n  'abbot',\n  'abbotsburi',\n  'abbotsford',\n  'abbott',\n  'abbottabad',\n  'abboud',\n  'abbr',\n  'abbrev',\n  'abbrevi',\n  'abc',\n  'abca',\n  'abcc',\n  'abcd',\n  'abcdefghijklmnopqrstuvwxyz',\n  'abcl',\n  'abco',\n  'abcp',\n  'abd',\n  'abdalla',\n  'abdallah',\n  'abdel',\n  'abdelatif',\n  'abdelaziz',\n  'abdelhamid',\n  'abdelkrim',\n  'abdella',\n  'abdelmalek',\n  'abdelrahman',\n  'abdelsalam',\n  'abdera',\n  'abderhalden',\n  'abdeslam',\n  'abdi',\n  'abdic',\n  'abdin',\n  'abdo',\n  'abdol',\n  'abdollah',\n  'abdomen',\n  'abdomin',\n  'abdomini',\n  'abdon',\n  'abdou',\n  'abdoul',\n  'abdoulay',\n  'abdu',\n  'abducen',\n  'abduct',\n  'abducte',\n  'abductor',\n  'abduh',\n  'abdul',\n  'abdula',\n  'abdulahi',\n  'abdulaziz',\n  'abdulla',\n  'abdullaev',\n  'abdullah',\n  'abdullahi',\n  'abdulmajid',\n  'abdulmutallab',\n  'abdulrahman',\n  'abdur',\n  'abdussalam',\n  'abe',\n  'abeam',\n  'abeba',\n  'abebook',\n  'abecedarian',\n  'abedi',\n  'abedin',\n  'abedini',\n  'abednego',\n  'abeer',\n  'abegweit',\n  'abeil',\n  'abel',\n  'abela',\n  'abelard',\n  'abelardo',\n  'abelia',\n  'abelian',\n  'abelisaurid',\n  'abella',\n  'abellio',\n  'abello',\n  'abelson',\n  'aben',\n  'abenaki',\n  'abend',\n  'abengoa',\n  'abeokuta',\n  'abequa',\n  'aber',\n  'aberconwi',\n  'abercorn',\n  'abercrombi',\n  'abercynon',\n  'aberdar',\n  'aberdeen',\n  'aberdeenshir',\n  'aberdour',\n  'aberfeldi',\n  'aberfoyl',\n  'abergavenni',\n  'abergel',\n  'aberlin',\n  'abern',\n  'abernathi',\n  'abernethi',\n  'aberr',\n  'aberra',\n  'abert',\n  'aberystwyth',\n  'abet',\n  'abettor',\n  'abey',\n  'abf',\n  'abg',\n  'abgeordnetenhau',\n  'abh',\n  'abha',\n  'abhainn',\n  'abhandlungen',\n  'abhay',\n  'abhaya',\n  'abhi',\n  'abhidhamma',\n  'abhijeet',\n  'abhijit',\n  'abhik',\n  'abhimanyu',\n  'abhinav',\n  'abhinavagupta',\n  'abhishek',\n  'abhor',\n  'abhorr',\n  'abi',\n  'abia',\n  'abian',\n  'abib',\n  'abid',\n  'abidin',\n  'abidjan',\n  'abierto',\n  'abigail',\n  'abijah',\n  'abil',\n  'abila',\n  'abildgaard',\n  'abilen',\n  'abilifi',\n  'abilio',\n  'abim',\n  'abimbola',\n  'abin',\n  'abing',\n  'abingdon',\n  'abington',\n  'abiogen',\n  'abiogenesi',\n  'abiola',\n  'abiom',\n  'abiot',\n  'abiquiu',\n  'abir',\n  'abisko',\n  'abit',\n  'abita',\n  'abitibi',\n  'abitur',\n  'abjad',\n  'abject',\n  'abjectli',\n  'abjur',\n  'abkhazia',\n  'abl',\n  'abla',\n  'ablat',\n  'ablaut',\n  'ablaz',\n  'ableman',\n  'abler',\n  'ableton',\n  'ablett',\n  'abli',\n  'abloom',\n  'abloy',\n  'ablut',\n  'abm',\n  'abn',\n  'abneg',\n  'abner',\n  'abnett',\n  'abney',\n  'abnorm',\n  'abnt',\n  'abo',\n  'aboard',\n  'abod',\n  'abogado',\n  'abolfazl',\n  'abolish',\n  'abolit',\n  'abolition',\n  'abolitionist',\n  'abomin',\n  'abond',\n  'abor',\n  'aborigin',\n  'aborn',\n  'abort',\n  'abortifaci',\n  'abortionist',\n  'abot',\n  'abott',\n  'abou',\n  'aboubakar',\n  'aboukir',\n  'abound',\n  'about',\n  'aboutaleb',\n  'aboveground',\n  'abovement',\n  'abp',\n  'abpp',\n  'abq',\n  'abqaiq',\n  'abr',\n  'abra',\n  'abraaj',\n  'abracadabra',\n  'abrad',\n  'abraha',\n  'abraham',\n  'abrahamian',\n  'abrahamowicz',\n  'abrahamson',\n  'abrahamsson',\n  'abram',\n  'abramoff',\n  'abramov',\n  'abramovich',\n  'abramowicz',\n  'abramowitz',\n  'abramski',\n  'abramson',\n  'abrant',\n  'abrar',\n  'abras',\n  'abravanel',\n  'abraxa',\n  'abraxan',\n  'abrazo',\n  'abreast',\n  'abrego',\n  'abren',\n  'abreu',\n  'abri',\n  'abric',\n  'abridg',\n  'abriel',\n  'abrikosov',\n  'abril',\n  'abrir',\n  'abroad',\n  'abrog',\n  'abrolho',\n  'abron',\n  'abrupt',\n  'abruptli',\n  'abruzzes',\n  'abruzzi',\n  'abruzzo',\n  'absa',\n  'absalom',\n  'absalon',\n  'absaroka',\n  'abscam',\n  'abscess',\n  'abscis',\n  'absciss',\n  'abscissa',\n  'abscond',\n  'absecon',\n  'abseil',\n  'absenc',\n  'absens',\n  'absent',\n  'absente',\n  'absentia',\n  'absher',\n  'absheron',\n  'abshir',\n  'absinth',\n  'absinthium',\n  'absolom',\n  'absolut',\n  'absolutepunk',\n  'absolutist',\n  'absoluto',\n  'absolv',\n  'absorb',\n  'absorpt',\n  'abssi',\n  'abstain',\n  'abstemi',\n  'abstent',\n  'abstin',\n  'abstract',\n  'abstraction',\n  'abstractionist',\n  'abstractli',\n  'abstrus',\n  'absurd',\n  'absurdist',\n  'absurdistan',\n  'absurdli',\n  'absurdum',\n  'abt',\n  'abta',\n  'abtahi',\n  'abteilung',\n  'abu',\n  'abubakar',\n  'abudu',\n  'abuela',\n  'abuelo',\n  'abuja',\n  'abul',\n  'abuna',\n  'abund',\n  'abunda',\n  'abundantli',\n  'aburi',\n  'abus',\n  'abut',\n  'abutilon',\n  'abuzz',\n  'abv',\n  'abvp',\n  'abw',\n  'abwehr',\n  'abx',\n  'abyan',\n  'abydo',\n  'abyei',\n  'abysm',\n  'abyss',\n  'abyssinia',\n  'abyssinian',\n  'abyssinica',\n  'abz',\n  'abzug',\n  'ac',\n  'aca',\n  'acac',\n  'acacia',\n  'acacio',\n  'acad',\n  'academ',\n  'academi',\n  'academia',\n  'academica',\n  'academician',\n  'acadi',\n  'acadia',\n  'acadian',\n  'acadiana',\n  'acadien',\n  'acai',\n  'acala',\n  'acalypha',\n  'acampros',\n  'acan',\n  'acanthacea',\n  'acanthamoeba',\n  'acanthosi',\n  'acanthostega',\n  'acanthu',\n  'acap',\n  'acapella',\n  'acappella',\n  'acapulco',\n  'acar',\n  'acara',\n  'acarbos',\n  'acari',\n  'acarida',\n  'acasta',\n  'acat',\n  'acauli',\n  'acb',\n  'acbar',\n  'acbl',\n  'acbsp',\n  'acc',\n  'acca',\n  'accademia',\n  'accardo',\n  'accc',\n  'acccord',\n  'acced',\n  'accel',\n  'acceler',\n  'accelerando',\n  'acceleromet',\n  'accent',\n  'accentor',\n  'accentu',\n  'accentur',\n  'accept',\n  'acceptor',\n  'acceso',\n  'access',\n  'accessor',\n  'accessori',\n  'accetta',\n  'acci',\n  'accid',\n  'accident',\n  'accio',\n  'accion',\n  'acciona',\n  'accipit',\n  'accipitrida',\n  'acclaim',\n  'acclam',\n  'acclim',\n  'acclimat',\n  'acclimatis',\n  'acco',\n  'accokeek',\n  'accol',\n  'accola',\n  'accolad',\n  'accomac',\n  'accomack',\n  'accommod',\n  'accomod',\n  'accompani',\n  'accompanist',\n  'accompli',\n  'accomplic',\n  'accomplish',\n  'accor',\n  'accord',\n  'accordian',\n  'accordingli',\n  'accordion',\n  'accordionist',\n  'accost',\n  'account',\n  'accountemp',\n  'accounthold',\n  'accouter',\n  'accoutr',\n  'accp',\n  'accra',\n  'accredit',\n  'accreditor',\n  'accret',\n  'accretionari',\n  'accrington',\n  'accross',\n  'accru',\n  'accrual',\n  'acct',\n  'accu',\n  'accultur',\n  'accum',\n  'accumben',\n  'accumul',\n  'accur',\n  'accuraci',\n  'accurs',\n  'accus',\n  'accusatori',\n  'accusingli',\n  'accustom',\n  'accutan',\n  'accuweath',\n  'acd',\n  'acda',\n  'acdc',\n  'acdelco',\n  'ace',\n  'acea',\n  'acec',\n  'aceh',\n  'acei',\n  'acela',\n  'acellular',\n  'acep',\n  'acer',\n  'acerb',\n  'acerca',\n  'acero',\n  'acess',\n  'acesulfam',\n  'acet',\n  'acetabular',\n  'acetabulum',\n  'acetaldehyd',\n  'acetaminophen',\n  'acetazolamid',\n  'aceto',\n  'acetoacet',\n  'acetobact',\n  'acetoin',\n  'aceton',\n  'acetonid',\n  'acetonitril',\n  'acetophenon',\n  'acetosa',\n  'acetosella',\n  'acetyl',\n  'acetylaceton',\n  'acetylcholin',\n  'acetylcholinesteras',\n  'acetylcystein',\n  'acetylen',\n  'acetylid',\n  'acetylsalicyl',\n  'acetyltransferas',\n  'acev',\n  'acevedo',\n  'acey',\n  'acf',\n  'acg',\n  'acgm',\n  'ach',\n  'acha',\n  'achaea',\n  'achaean',\n  'achaemenian',\n  'achaemenid',\n  'achaia',\n  'achala',\n  'achalasia',\n  'achan',\n  'achanta',\n  'achar',\n  'acharya',\n  'achat',\n  'achatina',\n  'acheampong',\n  'acheb',\n  'acheiv',\n  'achel',\n  'acheloo',\n  'achen',\n  'achenbach',\n  'achern',\n  'acheron',\n  'acheson',\n  'acheulean',\n  'acheulian',\n  'achi',\n  'achiev',\n  'achieva',\n  'achil',\n  'achillea',\n  'achilleo',\n  'achilleu',\n  'achim',\n  'achimota',\n  'achin',\n  'achingli',\n  'achiot',\n  'achir',\n  'achl',\n  'achm',\n  'acho',\n  'acholi',\n  'achon',\n  'achondrit',\n  'achondroplasia',\n  'achr',\n  'achrafieh',\n  'achromat',\n  'achromatopsia',\n  'achterhoek',\n  'achtung',\n  'achu',\n  'achuar',\n  'achuthanandan',\n  'aci',\n  'acia',\n  'acic',\n  'aciclovir',\n  'acicular',\n  'acid',\n  'acidemia',\n  'acidif',\n  'acidifi',\n  'acidophil',\n  'acidophilu',\n  'acidosi',\n  'acidul',\n  'aciduria',\n  'aciliu',\n  'acim',\n  'aciman',\n  'acinar',\n  'acinetobact',\n  'acinonyx',\n  'acip',\n  'acipens',\n  'acireal',\n  'acit',\n  'acj',\n  'ack',\n  'ackbar',\n  'acke',\n  'acker',\n  'ackerli',\n  'ackerman',\n  'ackermann',\n  'ackl',\n  'acklam',\n  'ackland',\n  'ackley',\n  'acklin',\n  'ackman',\n  'acknowledg',\n  'acknowleg',\n  'ackoff',\n  'ackroyd',\n  'ackworth',\n  'acl',\n  'aclara',\n  'aclj',\n  'aclu',\n  'acm',\n  'acma',\n  'acmi',\n  'acn',\n  'acna',\n  'aco',\n  'acog',\n  'acol',\n  'acolyt',\n  'acom',\n  'acoma',\n  'acomb',\n  'acon',\n  'aconcagua',\n  'aconit',\n  'aconitum',\n  'acord',\n  'acorda',\n  'acorn',\n  'acoru',\n  'acosta',\n  'acount',\n  'acoust',\n  'acp',\n  'acpa',\n  'acpi',\n  'acpo',\n  'acq',\n  'acqua',\n  'acquaint',\n  'acquaintanceship',\n  'acquaviva',\n  'acqui',\n  'acquiesc',\n  'acquir',\n  'acquisit',\n  'acquist',\n  'acquit',\n  'acquitt',\n  'acr',\n  'acra',\n  'acral',\n  'acreag',\n  'acri',\n  'acrid',\n  'acridida',\n  'acridin',\n  'acrimoni',\n  'acrisiu',\n  'acrl',\n  'acro',\n  'acrobat',\n  'acrolein',\n  'acromegali',\n  'acromion',\n  'acronicta',\n  'acronym',\n  'acrophobia',\n  'acropoli',\n  'acropora',\n  'acrosom',\n  'across',\n  'acrost',\n  'acryl',\n  'acrylamid',\n  'acrylonitril',\n  'acsa',\n  'acsi',\n  'acsm',\n  'acss',\n  'act',\n  'acta',\n  'actaea',\n  'actaeon',\n  'actavi',\n  'actblu',\n  'actc',\n  'actel',\n  'actelion',\n  'acth',\n  'actin',\n  'actinid',\n  'actinidia',\n  'actinidiacea',\n  'actinium',\n  'actinobacteria',\n  'actinolit',\n  'actinomorph',\n  'actinomyc',\n  'actinomycet',\n  'actio',\n  'action',\n  'actionaid',\n  'actionscript',\n  'actiq',\n  'actitud',\n  'actium',\n  'activ',\n  'activa',\n  'activas',\n  'activesync',\n  'activewear',\n  'activex',\n  'activia',\n  'actividad',\n  'activis',\n  'activist',\n  'activit',\n  'acto',\n  'acton',\n  'actonel',\n  'actor',\n  'actress',\n  'actu',\n  'actua',\n  'actual',\n  'actualidad',\n  'actualment',\n  'actuar',\n  'actuari',\n  'actuat',\n  'actuel',\n  'actv',\n  'acu',\n  'acuerdo',\n  'acuff',\n  'acuiti',\n  'aculeata',\n  'aculeatu',\n  'acum',\n  'acumen',\n  'acumin',\n  'acuminata',\n  'acuminatum',\n  'acuna',\n  'acupoint',\n  'acupressur',\n  'acupunctur',\n  'acupuncturist',\n  'acura',\n  'acushnet',\n  'acut',\n  'acuta',\n  'acutu',\n  'acuvu',\n  'acv',\n  'acw',\n  'acwf',\n  'acworth',\n  'acx',\n  'acxiom',\n  'acycl',\n  'acyclovir',\n  'acyl',\n  'acyltransferas',\n  'ad',\n  'ada',\n  'adab',\n  'adac',\n  'adachi',\n  'adad',\n  'adado',\n  'adag',\n  'adagio',\n  'adah',\n  'adai',\n  'adair',\n  'adairsvil',\n  'adak',\n  'adal',\n  'adalah',\n  'adalat',\n  'adalbert',\n  'adalberto',\n  'adalet',\n  'adalgisa',\n  'adalia',\n  'adalid',\n  'adalimumab',\n  'adalin',\n  'adam',\n  'adama',\n  'adamantan',\n  'adamantin',\n  'adamantium',\n  'adamantli',\n  'adamawa',\n  'adamczyk',\n  'adament',\n  'adaminabi',\n  'adamo',\n  'adamski',\n  'adamson',\n  'adamstown',\n  'adamsvil',\n  'adan',\n  'adana',\n  'adang',\n  ...],\n matrix([[ 8.85700556,  7.40211618,  7.94886249, ..., 11.52758065,\n          13.7962642 , 12.69765191]]))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_process_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188dd49-843b-4cf9-80ea-4db4c43a00d0",
   "metadata": {},
   "source": [
    "### TF-IDF Query Processing & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61123f4-3b40-486c-9da3-5bfa1e717171",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:41.884185Z",
     "start_time": "2023-10-21T21:15:41.857370300Z"
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_vectorize_queries(queries_df, vocabulary, idf):\n",
    "    \"\"\"\n",
    "    Convert each query in the DataFrame into its TF-IDF vector.\n",
    "\n",
    "    Args:\n",
    "        queries_df: DataFrame containing 'text' column with raw queries.\n",
    "        vocabulary: List of unique terms used for vectorization.\n",
    "        idf: Array containing Inverse Document Frequencies for each term.\n",
    "\n",
    "    Returns:\n",
    "        tfidf_matrix: TF-IDF matrix of shape (num_queries, num_terms).\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Process queries ...\")\n",
    "    # Preprocess all queries\n",
    "    queries_df['processed'] = queries_df['text'].apply(preprocess_text)\n",
    "\n",
    "    print(\"Initialize sparse matrix ...\")\n",
    "    num_queries = len(queries_df)\n",
    "    num_terms = len(vocabulary)\n",
    "\n",
    "    # Using a dictionary for term index lookup\n",
    "    vocab_dict = {term: index for index, term in enumerate(vocabulary)}\n",
    "    tf_matrix = lil_matrix((num_queries, num_terms))\n",
    "\n",
    "    print(\"Compute  tf ...\")\n",
    "    # Populate the sparse matrix\n",
    "    for idx, row in queries_df.iterrows():\n",
    "        for term in row['processed']:\n",
    "            if term in vocab_dict:\n",
    "                tf_matrix[idx, vocab_dict[term]] += 1\n",
    "\n",
    "    print(\"Multiply by idf ...\")\n",
    "    # Convert to CSR format for efficient multiplication and transform TFs to TF-IDF\n",
    "    tfidf_matrix = (tf_matrix.tocsr()).multiply(idf)\n",
    "\n",
    "    print(\"Done !\")\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc26491-b76f-4bd0-8bbf-21be5b0b4497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:41.992579700Z",
     "start_time": "2023-10-21T21:15:41.874523300Z"
    }
   },
   "outputs": [],
   "source": [
    "def top_k_indices(matrix: csr_matrix, k: int):\n",
    "    \"\"\"\n",
    "    Get top k indices for each row of a sparse matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix (scipy.sparse.csr_matrix): Input sparse matrix.\n",
    "        k (int): Number of top indices to retrieve for each row.\n",
    "\n",
    "    Returns:\n",
    "        top_indices: List of lists containing the top k indices for each row of the input matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Placeholder list for top k indices for each row\n",
    "    top_indices = []\n",
    "\n",
    "    # Iterate over each row\n",
    "    print('Iterate over each row ...')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        row_data = matrix.data[matrix.indptr[i]:matrix.indptr[i + 1]]\n",
    "        row_indices = matrix.indices[matrix.indptr[i]:matrix.indptr[i + 1]]\n",
    "\n",
    "        if len(row_data) < k:\n",
    "            top_indices.append(row_indices)\n",
    "        else:\n",
    "            # Sort the row data and get top k indices\n",
    "            sorted_indices = np.argsort(-row_data)\n",
    "            top_indices.append(row_indices[sorted_indices[:k]])\n",
    "\n",
    "\n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c67d459-c5be-4d40-a726-143970e9edd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:42.019177700Z",
     "start_time": "2023-10-21T21:15:41.902657100Z"
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_predict_documents(tfidf_matrix_normalized, query_vectors, k):\n",
    "    \"\"\"\n",
    "    Process multiple queries and return ranked document indices for each query.\n",
    "\n",
    "    Args:\n",
    "        tfidf_matrix_normalized (scipy.sparse.csr_matrix): Normalized TF-IDF matrix of documents.\n",
    "        query_vectors (numpy.array): TF-IDF vectors of query documents.\n",
    "        k (int): Number of top-ranked documents to retrieve for each query.\n",
    "\n",
    "    Returns:\n",
    "        ranked_doc_indices: List of lists containing the top k ranked document indices for each query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute cosine similarities using matrix operations\n",
    "    print(\"Compute cosine similarities ...\")\n",
    "    similarity_matrix = cosine_similarity(query_vectors, tfidf_matrix_normalized, dense_output=False)\n",
    "\n",
    "    # Get document indices ranked by relevance for each query\n",
    "    print(\"Rank documents ...\")\n",
    "    # print(similarity_matrix.shape)\n",
    "    # ranked_doc_indices = np.argsort(-similarity_matrix)[:, :k]\n",
    "    ranked_doc_indices = top_k_indices(similarity_matrix, k)\n",
    "\n",
    "    return ranked_doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "716fffe9-ddfe-4228-ba45-999b4f693b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:42.021208700Z",
     "start_time": "2023-10-21T21:15:41.920048Z"
    }
   },
   "outputs": [],
   "source": [
    "def predictions_to_ids_ranking(corpus, queries, prediction):\n",
    "    \"\"\"\n",
    "    Map prediction rows to corresponding 'corpus-id' values from the corpus and create a DataFrame with 'id',\n",
    "    'corpus-id', and 'score' columns.\n",
    "\n",
    "    Args:\n",
    "        corpus (pandas.DataFrame): DataFrame containing 'corpus-id' values.\n",
    "        queries (pandas.DataFrame): DataFrame containing 'query-id' values.\n",
    "        prediction (list): List of lists containing top ranked 'corpus-id' values for each query.\n",
    "\n",
    "    Returns:\n",
    "        ids_ranking: DataFrame with 'id', 'corpus-id', and 'score' columns representing ranked predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Map the prediction rows to the corresponding 'corpus-id' values from the corpus\n",
    "    mapped_results = [corpus.iloc[row]['corpus-id'].values.tolist() for row in prediction]\n",
    "\n",
    "    # Create a DataFrame with 'id', 'corpus-id', and 'score' columns\n",
    "    ids_ranking = pd.DataFrame({\n",
    "        'id': queries['query-id'].iloc[:len(mapped_results)],\n",
    "        'corpus-id': mapped_results,\n",
    "        'score': [-1 for _ in range(len(mapped_results))]\n",
    "    })\n",
    "\n",
    "    return ids_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b82fc-dab5-476a-90eb-c0ce2f6fff35",
   "metadata": {},
   "source": [
    "### Deep Embedder Corpus Processing\n",
    "We will use the [SentenceTransformer](https://www.sbert.net/) library to generate embeddings for the corpus.\n",
    "For this, we will use the [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:42.022203500Z",
     "start_time": "2023-10-21T21:15:41.934119400Z"
    }
   },
   "id": "6680085b8ae0ab39"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To avoid re-computing the embeddings every time, we will save the computed embeddings to disk using [pickle](https://docs.python.org/3/library/pickle.html)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "392311f25179865e"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7e3632d-3fab-4b1d-898a-f2c1fa2373e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:42.022203500Z",
     "start_time": "2023-10-21T21:15:41.953404900Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pretrained_embedder():\n",
    "    try:\n",
    "        with open(f'{DATA_FOLDER}deep_embedder.pkl', 'rb') as f:\n",
    "            deep_embedder = pickle.load(f)\n",
    "        return deep_embedder\n",
    "    except:\n",
    "        print('404, Fetching deep embedder')\n",
    "        deep_embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        with open(f'{DATA_FOLDER}deep_embedder.pkl', 'wb') as f:\n",
    "            pickle.dump(deep_embedder, f)\n",
    "        return deep_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf937e7d-bd09-4dae-9d62-68804e3ad2e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:42.361130300Z",
     "start_time": "2023-10-21T21:15:41.960470200Z"
    }
   },
   "outputs": [],
   "source": [
    "DEEP_EMBEDDER = load_pretrained_embedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the [encode](https://www.sbert.net/docs/package_reference/encoders.html#sentence-transformers-encode) method of the SentenceTransformer class to generate embeddings for the corpus.\n",
    "To avoid running out of memory, we will process the corpus in batches.\n",
    "To avoid re-computing the embeddings every time, we will save the computed embeddings to disk using [pickle](https://docs.python.org/3/library/pickle.html)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c227912d2cc46b9"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41a0d7a1-7916-4e06-a55d-e2362c462c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:42.402295600Z",
     "start_time": "2023-10-21T21:15:42.369644Z"
    }
   },
   "outputs": [],
   "source": [
    "def deep_embedder_process_corpus():\n",
    "    try:\n",
    "        with open(f'{DATA_FOLDER}vectorized_corpus.pkl', 'rb') as f:\n",
    "            embedded_corpus = pickle.load(f)\n",
    "        return embedded_corpus\n",
    "    except:\n",
    "        print('404, Computing Embeded Corpus ...')\n",
    "        embedded_corpus = DEEP_EMBEDDER.encode(sentences=corpus[\"text\"].tolist(),\n",
    "                                               batch_size=500,  # TO BE CHANGED\n",
    "                                               show_progress_bar=True,\n",
    "                                               device='cpu',  # TO BE CHANGED -- 'cpu', 'cuda', automatic if None\n",
    "                                               )\n",
    "\n",
    "        with open(f'{DATA_FOLDER}vectorized_corpus.pkl', 'wb') as f:\n",
    "            pickle.dump(embedded_corpus, f)\n",
    "        return embedded_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a796a-5fa0-4176-9460-92b4931a8f50",
   "metadata": {},
   "source": [
    "### Deep Embedder Query Processing & Prediction\n",
    "We will use the [encode](https://www.sbert.net/docs/package_reference/encoders.html#sentence-transformers-encode) method of the SentenceTransformer class to generate embeddings for the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8237bc28-6a5e-4ec9-a401-1b76c66a8510",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:42.402295600Z",
     "start_time": "2023-10-21T21:15:42.383258Z"
    }
   },
   "outputs": [],
   "source": [
    "def deep_vectorize_queries(queries):\n",
    "    return DEEP_EMBEDDER.encode(queries.text.tolist(),\n",
    "                                batch_size=500,  # TO BE CHANGED \n",
    "                                show_progress_bar=True,\n",
    "                                device='cpu',  # TO BE CHANGED -- 'cpu', 'cuda', automatic if None\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "844cda11-52d8-4d6b-8ec5-653c2179a7e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:15:42.446600400Z",
     "start_time": "2023-10-21T21:15:42.391058200Z"
    }
   },
   "outputs": [],
   "source": [
    "def deep_predict_documents(top_large_k, vectorized_queries, vectorized_corpus, k=10):\n",
    "    \"\"\"\n",
    "    Predict relevant documents for each query using deep embeddings.\n",
    "\n",
    "    Args:\n",
    "        top_large_k: 2D array containing top candidates for each query.\n",
    "        vectorized_queries: Array of query embeddings.\n",
    "        vectorized_corpus: DataFrame containing embeddings of the corpus.\n",
    "        k (int, optional): Number of top-ranked documents to retrieve for each query. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        top_k: 2D array containing indices of relevant documents for each query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 2D Array for storing indices to relevant documents\n",
    "    # Shape (Number of queries, k)\n",
    "    top_k = np.zeros((vectorized_queries.shape[0], k))\n",
    "\n",
    "    # Iterate through each query embedding\n",
    "    for idx, vector_query in enumerate(vectorized_queries):\n",
    "        # Index the embedding of relevant candidates\n",
    "        # Shape of sentence_feature: (large_k, 384)\n",
    "        sentence_feature = vectorized_corpus.loc[top_large_k[idx]]  \n",
    "\n",
    "        # Dot product (numerator of cosine similarity), similar to linear_kernel\n",
    "        similarity = sentence_feature @ vector_query\n",
    "\n",
    "        # Get indices of top-k highest similarities\n",
    "        top_k[idx] = np.argsort(similarity)[-k:]\n",
    "    return top_k.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1\n",
    "Execution cell for task-1 documents retrieval using TF-IDF and deep embeddings.\n",
    "We also track time taken for documents retrieval."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca2fc2a5fc7a77bd"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afe635869b7f99f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T21:21:49.732208900Z",
     "start_time": "2023-10-21T21:15:42.409165200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process queries ...\n",
      "Initialize sparse matrix ...\n",
      "Compute  tf ...\n",
      "Multiply by idf ...\n",
      "Done !\n",
      "Compute cosine similarities ...\n",
      "Rank documents ...\n",
      "Iterate over each row ...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68a1c27806b24076a775d91b9f06fe95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 36s\n",
      "Wall time: 6min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## TF-IDF PREDICTION \n",
    "k = 5000\n",
    "documents, tf_idf, vocabulary, idf = tfidf_process_corpus()\n",
    "tfidf_query_vectors = tfidf_vectorize_queries(queries_test, vocabulary, idf)\n",
    "prediction = tfidf_predict_documents(tf_idf, tfidf_query_vectors, k)\n",
    "map_ = predictions_to_ids_ranking(corpus, queries_test, prediction)\n",
    "\n",
    "## DEEP EMBEDDING PREDICTION\n",
    "VECTORIZED_CORPUS = deep_embedder_process_corpus()\n",
    "VECTORIZED_CORPUS = pd.DataFrame(VECTORIZED_CORPUS, index=corpus['corpus-id'])\n",
    "\n",
    "top_large_k = np.zeros(shape=(map_.shape[0], k))\n",
    "for i in range(map_.shape[0]):\n",
    "    new_line = np.array(map_.iloc[i]['corpus-id'])\n",
    "    for j in range(len(new_line)):\n",
    "        top_large_k[i][j] = new_line[j]\n",
    "\n",
    "top_large_k = top_large_k.astype(int)\n",
    "deep_vectors = deep_vectorize_queries(queries_test)\n",
    "\n",
    "top10 = deep_predict_documents(top_large_k, deep_vectors, VECTORIZED_CORPUS, 10)\n",
    "top10 = pd.DataFrame(top10)\n",
    "\n",
    "task_1_result = pd.DataFrame(columns=['corpus-id', 'score'])\n",
    "\n",
    "for i in range(top10.shape[0]):\n",
    "    new_top = [top_large_k[i][top10.iloc[i]].tolist()]\n",
    "    task_1_result.loc[i] = new_top + [-1]\n",
    "    \n",
    "# task_1_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 2\n",
    "Execution cell for task-2 documents reranking using previously calculated and stored TF-IDF results.\n",
    "We also track time taken for documents reranking."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "543e934f1ae891c1"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process queries ...\n",
      "Initialize sparse matrix ...\n",
      "Compute  tf ...\n",
      "Multiply by idf ...\n",
      "Done !\n",
      "CPU times: total: 16.9 s\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Get a list of mappings from corpus-id to index in the TF-IDF matrix\n",
    "corpus_ids_mapping = []\n",
    "for row in df_task_2.iloc:\n",
    "    corpus_ids_indices = []\n",
    "    for corpus_id in row['corpus-id']:\n",
    "        corpus_ids_indices.append(corpus.index[corpus['corpus-id'] == int(corpus_id)][0])\n",
    "    corpus_ids_mapping.append(corpus_ids_indices)\n",
    "\n",
    "# Get the TF-IDF matrix for queries       \n",
    "vectorized = tfidf_vectorize_queries(df_task_2, vocabulary, idf).tocsr()\n",
    "relevant_scores = []\n",
    "\n",
    "# Compute relevance scores as cosine similarity between each query and its relevant documents\n",
    "for idx, vector_query in enumerate(vectorized):\n",
    "    docc = tf_idf.tocsr()[corpus_ids_mapping[idx]]\n",
    "    similarity = cosine_similarity(vector_query, docc).flatten()\n",
    "\n",
    "    relevant_scores.append(similarity.tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T21:22:28.388987900Z",
     "start_time": "2023-10-21T21:21:49.725407700Z"
    }
   },
   "id": "f586d2e4502feb18"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "df_task_2['score'] = relevant_scores\n",
    "df_task_2['corpus-id'] = -1\n",
    "\n",
    "task_2_result = df_task_2.drop(columns=['processed', 'text', 'id'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T21:22:28.449134100Z",
     "start_time": "2023-10-21T21:22:28.395977100Z"
    }
   },
   "id": "e6ec8d82345f391c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submission\n",
    "Here we concatenate the results of task 1 and task 2 and save the final submission file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4951105d7ed7cf5c"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def submission_to_csv(submission):\n",
    "    \"\"\"\n",
    "    Save the submission DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        submission (pandas.DataFrame): DataFrame containing submission data.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    DATA_FOLDER = \"data/\"\n",
    "    FILE_NAME = \"submission\"\n",
    "    submission.index.name='id'\n",
    "    submission.to_csv(f'{DATA_FOLDER}{FILE_NAME}.csv', index=True, header=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T21:22:58.567798200Z",
     "start_time": "2023-10-21T21:22:58.550072400Z"
    }
   },
   "id": "280cc1d4b4f8720c"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Concatenation with result of task 2\n",
    "result = pd.concat([task_1_result, task_2_result], axis=0).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T21:23:01.110235100Z",
     "start_time": "2023-10-21T21:23:01.087519700Z"
    }
   },
   "id": "474efc2dd08a2a91"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "submission_to_csv(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T21:23:03.239262200Z",
     "start_time": "2023-10-21T21:23:03.180779700Z"
    }
   },
   "id": "d1cf366ae9383e18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
